# ðŸ¤– AGENT PRESETS CATALOG - VENTROS AI

> **CatÃ¡logo completo de presets de agentes profissionais usando Google ADK**
> PadrÃµes avanÃ§ados: Medical, Research, Meta-Analysis, Data Science, Legal, Financial

---

## ðŸ“‹ ÃNDICE

1. [Agentes Existentes](#agentes-existentes)
2. [Novos Agentes Profissionais](#novos-agentes-profissionais)
3. [EstratÃ©gias de ComposiÃ§Ã£o ADK](#estratÃ©gias-de-composiÃ§Ã£o-adk)
4. [PadrÃµes de Projeto Python](#padrÃµes-de-projeto-python)
5. [ImplementaÃ§Ã£o Detalhada](#implementaÃ§Ã£o-detalhada)

---

## âœ… AGENTES EXISTENTES

### **Agentes BÃ¡sicos (Pattern Examples)**

| Agent | Type | Pattern | Use Case |
|-------|------|---------|----------|
| **RetentionChurnAgent** | LlmAgent | ReAct | PrevenÃ§Ã£o de churn |
| **LeadEnrichmentAgent** | ParallelAgent | Concurrent | Enriquecimento paralelo |
| **QualityAssuranceAgent** | LoopAgent | Iterative | QA atÃ© critÃ©rios |
| **CoordinatorAgent** | LlmAgent | Orchestrator | Roteamento especialistas |
| **SupportAgent** | LlmAgent | Handoff | Suporte com transferÃªncia |
| **ReflectiveAgent** | LlmAgent | Self-Critique | Auto-avaliaÃ§Ã£o |

### **Agentes de NegÃ³cio (Production)**

| Agent | Lookback | Tools | Strategy |
|-------|----------|-------|----------|
| **BIManagerAgent** | 365 dias | BigQuery analytics, MCP | Quantitative + Qualitative |
| **SDRAgent** | 30 dias | BANT qualification, lead scoring | Sales-focused hybrid |
| **AgentAnalyzerAgent** | 90 dias | Grammar, tone, brand alignment | Quality + comparison |

**Total atual: 12 agentes**

---

## ðŸš€ NOVOS AGENTES PROFISSIONAIS

### **Categoria 1: Research & Analysis**

#### **1. DeepResearchAgent** ðŸ“š

**PadrÃ£o ADK:** SequentialAgent â†’ ParallelAgent â†’ LoopAgent (iterative refinement)

```
FLUXO:
1. Query Analysis (LlmAgent)
   â†’ DecomposiÃ§Ã£o da pergunta de pesquisa
   â†’ IdentificaÃ§Ã£o de sub-questÃµes

2. Parallel Literature Search (ParallelAgent)
   â†’ PubMed/Google Scholar search
   â†’ Document vectorization
   â†’ Citation network analysis

3. Evidence Synthesis (LoopAgent)
   â†’ Iterative refinement atÃ© consensus
   â†’ Contradiction detection
   â†’ Quality scoring

4. Report Generation (LlmAgent)
   â†’ Structured output (Introduction, Methods, Results, Discussion)
   â†’ Citation management
```

**KnowledgeScope:**
```python
{
    "lookback_days": 730,  # 2 years for research
    "include_documents": True,
    "include_citations": True,
    "include_contradictions": True,
    "content_types": ["document", "research_paper", "article"],
    "quality_threshold": 0.8  # High quality only
}
```

**MCP Tools:**
- `search_documents` (vector + keyword)
- `get_document_references` (citation network)
- `analyze_document_trends` (BigQuery analytics)
- `extract_entities` (NER for researchers, institutions)

**Use Cases:**
- "O que dizem os Ãºltimos estudos sobre [X]?"
- "Quais evidÃªncias existem para [Y]?"
- "Compare a eficÃ¡cia de [tratamento A] vs [tratamento B]"

---

#### **2. MetaAnalysisAgent** ðŸ“Š

**PadrÃ£o ADK:** ParallelAgent (data extraction) â†’ LlmAgent (statistical synthesis) â†’ ReflectiveAgent (bias detection)

```
FLUXO:
1. Study Selection (LlmAgent)
   â†’ Apply inclusion/exclusion criteria
   â†’ PRISMA flow diagram generation

2. Parallel Data Extraction (ParallelAgent)
   â†’ Extract effect sizes from multiple studies
   â†’ Extract sample sizes, CI, p-values
   â†’ Risk of bias assessment

3. Statistical Pooling (LlmAgent + External R/Python)
   â†’ Fixed/random effects model
   â†’ Heterogeneity analysis (IÂ², TauÂ²)
   â†’ Subgroup analysis

4. Bias Detection (ReflectiveAgent)
   â†’ Publication bias (funnel plot)
   â†’ Sensitivity analysis
   â†’ GRADE quality assessment
```

**KnowledgeScope:**
```python
{
    "lookback_days": 3650,  # 10 years
    "include_documents": True,
    "include_statistical_data": True,
    "content_types": ["rct", "cohort", "case_control"],
    "min_quality_score": 7,  # Newcastle-Ottawa Scale
    "require_peer_reviewed": True
}
```

**Ferramentas Especializadas:**
- R Integration (metafor package)
- Python Integration (statsmodels, scipy)
- RevMan format export
- PRISMA checklist

**Outputs:**
```python
{
    "pooled_effect_size": {
        "estimate": 0.64,
        "ci_lower": 0.52,
        "ci_upper": 0.76,
        "p_value": 0.001,
        "model": "random_effects"
    },
    "heterogeneity": {
        "I2": 45.2,  # Moderate
        "Tau2": 0.03,
        "Q_statistic": 23.4,
        "p_value": 0.05
    },
    "bias_assessment": {
        "egger_test_p": 0.23,  # No publication bias
        "grade_quality": "moderate"
    },
    "forest_plot_url": "s3://...",
    "funnel_plot_url": "s3://..."
}
```

---

#### **3. ScientificHypothesisAgent** ðŸ§ª

**PadrÃ£o ADK:** ReflectiveAgent (hypothesis generation) â†’ ParallelAgent (validation checks) â†’ LoopAgent (refinement)

```
FLUXO:
1. Hypothesis Generation (ReflectiveAgent)
   â†’ Analyze existing data/literature
   â†’ Generate testable hypotheses
   â†’ Self-critique: Is it falsifiable?

2. Parallel Validation (ParallelAgent)
   â†’ Literature search: Already tested?
   â†’ Feasibility check: Available data?
   â†’ Ethics check: Is it ethical?
   â†’ Statistical power check: Sample size needed?

3. Refinement Loop (LoopAgent)
   â†’ Iterate until hypothesis is robust
   â†’ Add operational definitions
   â†’ Define success criteria

4. Study Design Proposal (LlmAgent)
   â†’ Experimental design (RCT, cohort, etc)
   â†’ Sample size calculation
   â†’ Statistical analysis plan
```

**Use Cases:**
- "Baseado nos dados de [X], que hipÃ³tese podemos testar?"
- "Como desenhar um estudo para testar [Y]?"
- "Essa hipÃ³tese jÃ¡ foi testada antes?"

---

### **Categoria 2: Medical & Healthcare**

#### **4. ClinicalTriageAgent** ðŸ¥

**PadrÃ£o ADK:** LlmAgent (symptom analysis) â†’ ParallelAgent (risk scoring) â†’ CoordinatorAgent (specialist routing)

```
FLUXO:
1. Symptom Collection (LlmAgent)
   â†’ Structured interview (SOAP format)
   â†’ Timeline construction
   â†’ Severity scoring

2. Risk Assessment (ParallelAgent)
   â†’ Cardiac risk score (HEART, TIMI)
   â†’ Sepsis risk (qSOFA)
   â†’ Stroke risk (FAST, ABCD2)
   â†’ Fall risk (Morse Scale)

3. Specialist Routing (CoordinatorAgent)
   â†’ IF emergency: Route to ER
   â†’ IF specialist needed: Book appointment
   â†’ IF self-care: Provide guidance
```

**IMPORTANTE - Compliance:**
```python
class ClinicalTriageAgent(LlmAgent):
    """
    âš ï¸ ESTE AGENTE Ã‰ PARA TRIAGEM ADMINISTRATIVA APENAS

    NÃƒO Ã‰:
    - DiagnÃ³stico mÃ©dico
    - PrescriÃ§Ã£o de tratamento
    - SubstituiÃ§Ã£o de consulta mÃ©dica

    Ã‰:
    - Sistema de triagem administrativa
    - ReduÃ§Ã£o de carga em call centers
    - PrÃ©-seleÃ§Ã£o de pacientes para telemedicina

    COMPLIANCE:
    - HIPAA/LGPD compliant
    - Logs auditÃ¡veis
    - Disclaimer obrigatÃ³rio em toda interaÃ§Ã£o
    - Human-in-the-loop para decisÃµes crÃ­ticas
    """
```

**KnowledgeScope:**
```python
{
    "lookback_days": 365,
    "include_medical_history": True,
    "include_medications": True,
    "include_allergies": True,
    "pii_masking": True,  # Mask CPF, address in logs
    "require_consent": True  # Explicit consent
}
```

**Risk Scoring Tools:**
```python
# Cardiac risk (HEART Score)
def calculate_heart_score(history, ecg, age, risk_factors, troponin):
    return {
        "score": 5,  # 0-10
        "risk": "moderate",  # low/moderate/high
        "recommendation": "ED evaluation within 6h",
        "rationale": "Troponin elevation + risk factors"
    }

# Sepsis risk (qSOFA)
def calculate_qsofa(resp_rate, mental_status, sbp):
    score = 0
    if resp_rate >= 22: score += 1
    if mental_status != "alert": score += 1
    if sbp <= 100: score += 1

    return {
        "score": score,  # 0-3
        "risk": "high" if score >= 2 else "low",
        "recommendation": "Immediate sepsis protocol" if score >= 2 else "Monitor"
    }
```

---

#### **5. MedicalLiteratureAgent** ðŸ“–

**PadrÃ£o ADK:** DeepResearchAgent (specialized for medical literature)

```
ESPECIALIZAÃ‡Ã•ES:
- PubMed/MEDLINE integration
- MeSH term extraction
- Clinical trial registry lookup (ClinicalTrials.gov)
- FDA drug approval database
- Cochrane systematic reviews
```

**Query Examples:**
- "Qual a eficÃ¡cia do [medicamento X] para [condiÃ§Ã£o Y]?"
- "Quais os efeitos colaterais reportados de [Z]?"
- "Existe evidÃªncia de superioridade entre [A] e [B]?"

---

### **Categoria 3: Legal & Compliance**

#### **6. ContractAnalyzerAgent** ðŸ“œ

**PadrÃ£o ADK:** SequentialAgent (clause extraction) â†’ ParallelAgent (risk analysis) â†’ LlmAgent (summary)

```
FLUXO:
1. Document Parsing (LlmAgent)
   â†’ Extract clauses
   â†’ Identify parties
   â†’ Extract key dates (vigÃªncia, renovaÃ§Ã£o)
   â†’ Extract financial terms (valor, multa, reajuste)

2. Risk Analysis (ParallelAgent)
   â†’ Unfavorable clause detection
   â†’ Missing clause detection (e.g., LGPD terms)
   â†’ Compliance check (labor law, LGPD, etc)
   â†’ Benchmark comparison (vs industry standard)

3. Summary Generation (LlmAgent)
   â†’ Executive summary
   â†’ Risk matrix (probability x impact)
   â†’ Recommended changes
```

**KnowledgeScope:**
```python
{
    "lookback_days": 1825,  # 5 years
    "include_documents": True,
    "content_types": ["contract", "agreement", "proposal"],
    "jurisdiction": "BR",  # Brazilian law
    "industry_sector": "technology"  # For benchmarking
}
```

**Risk Categories:**
```python
RISK_CATEGORIES = {
    "financial": [
        "unlimited_liability",
        "unfavorable_payment_terms",
        "high_penalties",
        "no_price_adjustment"
    ],
    "legal": [
        "missing_lgpd_clause",
        "unfavorable_jurisdiction",
        "unclear_termination",
        "no_confidentiality"
    ],
    "operational": [
        "unclear_sla",
        "no_force_majeure",
        "unrealistic_deadlines",
        "missing_ip_clause"
    ]
}
```

**Output Example:**
```json
{
    "contract_id": "CONT-2025-005",
    "parties": ["Company A LTDA", "JoÃ£o Silva"],
    "value": "R$ 10.000,00/mÃªs",
    "duration": "12 meses",
    "risk_summary": {
        "overall_risk": "medium",
        "financial_risk": "low",
        "legal_risk": "medium",
        "operational_risk": "medium"
    },
    "flagged_clauses": [
        {
            "clause": "ClÃ¡usula 5.2 - Responsabilidade",
            "issue": "Responsabilidade ilimitada da contratada",
            "severity": "high",
            "recommendation": "Adicionar cap de responsabilidade (ex: 12x valor mensal)"
        },
        {
            "clause": "Missing",
            "issue": "NÃ£o hÃ¡ clÃ¡usula LGPD explÃ­cita",
            "severity": "medium",
            "recommendation": "Adicionar clÃ¡usula de tratamento de dados conforme LGPD"
        }
    ],
    "favorable_terms": [
        "Prazo de pagamento NET30",
        "Reajuste anual por IPCA",
        "ClÃ¡usula de rescisÃ£o com 30 dias"
    ]
}
```

---

#### **7. LegalResearchAgent** âš–ï¸

**PadrÃ£o ADK:** DeepResearchAgent (specialized for legal research)

```
ESPECIALIZAÃ‡Ã•ES:
- JurisprudÃªncia search (STF, STJ, TRF)
- Lei/CÃ³digo lookup (Planalto)
- SÃºmulas e informativos
- Doutrina (livros, artigos)
```

**Query Examples:**
- "Qual o entendimento do STJ sobre [X]?"
- "Quais precedentes existem para [Y]?"
- "O que a doutrina diz sobre [Z]?"

---

### **Categoria 4: Financial & Investment**

#### **8. FinancialAnalystAgent** ðŸ’°

**PadrÃ£o ADK:** ParallelAgent (data fetching) â†’ LlmAgent (analysis) â†’ ReflectiveAgent (risk assessment)

```
FLUXO:
1. Parallel Data Fetching (ParallelAgent)
   â†’ Financial statements (BalanÃ§o, DRE)
   â†’ Market data (stock prices, volume)
   â†’ Economic indicators (Selic, IPCA, cÃ¢mbio)
   â†’ Competitor data

2. Financial Analysis (LlmAgent)
   â†’ Ratio analysis (ROE, ROA, P/E, EV/EBITDA)
   â†’ Trend analysis (YoY growth)
   â†’ Peer comparison
   â†’ DCF valuation

3. Risk Assessment (ReflectiveAgent)
   â†’ Market risk (beta, volatility)
   â†’ Credit risk (Altman Z-score)
   â†’ Liquidity risk (current ratio, quick ratio)
   â†’ Operational risk
```

**KnowledgeScope:**
```python
{
    "lookback_days": 1825,  # 5 years historical
    "include_financial_statements": True,
    "include_market_data": True,
    "include_macroeconomic": True,
    "data_sources": ["B3", "CVM", "Bloomberg", "Reuters"]
}
```

**Outputs:**
```python
{
    "company": "Company A SA",
    "ticker": "CMPA3",
    "recommendation": "buy",  # buy/hold/sell
    "target_price": 45.00,
    "current_price": 38.50,
    "upside_potential": "16.9%",

    "valuation": {
        "method": "DCF",
        "fair_value": 45.00,
        "assumptions": {
            "wacc": 0.12,
            "terminal_growth": 0.03,
            "projection_years": 10
        }
    },

    "ratios": {
        "p_e": 15.2,
        "p_b": 2.1,
        "roe": 18.5,
        "debt_equity": 0.4,
        "current_ratio": 1.8
    },

    "risks": [
        {
            "type": "market",
            "description": "Alta volatilidade (beta 1.4)",
            "severity": "medium"
        },
        {
            "type": "operational",
            "description": "ConcentraÃ§Ã£o em 1 cliente (40% receita)",
            "severity": "high"
        }
    ],

    "catalysts": [
        "ExpansÃ£o internacional prevista para Q2",
        "LanÃ§amento novo produto em Q3",
        "ReduÃ§Ã£o Selic favorece setor"
    ]
}
```

---

#### **9. InvestmentPortfolioAgent** ðŸ“ˆ

**PadrÃ£o ADK:** ParallelAgent (optimization) â†’ LlmAgent (recommendation) â†’ LoopAgent (rebalancing)

```
FLUXO:
1. Portfolio Analysis (LlmAgent)
   â†’ Current allocation
   â†’ Risk/return profile
   â†’ Diversification metrics (Sharpe, Sortino)

2. Parallel Optimization (ParallelAgent)
   â†’ Mean-variance optimization (Markowitz)
   â†’ Risk parity
   â†’ Black-Litterman
   â†’ Monte Carlo simulation

3. Rebalancing Loop (LoopAgent)
   â†’ Quarterly rebalancing
   â†’ Drift monitoring
   â†’ Tax-loss harvesting
   â†’ Transaction cost optimization
```

---

### **Categoria 5: Data Science & Analytics**

#### **10. DataAnalystAgent** ðŸ“Š

**PadrÃ£o ADK:** LlmAgent (query understanding) â†’ ParallelAgent (data fetching) â†’ LlmAgent (insight generation)

```
FLUXO:
1. Natural Language to SQL (LlmAgent)
   â†’ "Quantos leads tive este mÃªs?"
   â†’ SELECT COUNT(*) FROM contacts WHERE created_at >= '2025-01-01' AND type = 'lead'

2. Query Execution (ParallelAgent)
   â†’ PostgreSQL (operational data)
   â†’ BigQuery (analytical data)
   â†’ Redis (cache)

3. Insight Generation (LlmAgent)
   â†’ Statistical summary
   â†’ Trend detection
   â†’ Anomaly detection
   â†’ Visualization recommendation
```

**Query Types:**
```python
QUERY_TYPES = {
    "descriptive": "O que aconteceu?",  # COUNT, SUM, AVG
    "diagnostic": "Por que aconteceu?",  # GROUP BY, WHERE, correlation
    "predictive": "O que vai acontecer?",  # Time series, regression
    "prescriptive": "O que fazer?"  # Recommendation, optimization
}
```

---

#### **11. ABTestAgent** ðŸ§ª

**PadrÃ£o ADK:** SequentialAgent (experiment design) â†’ LoopAgent (monitoring) â†’ LlmAgent (decision)

```
FLUXO:
1. Experiment Design (LlmAgent)
   â†’ Define hypothesis
   â†’ Calculate sample size (power analysis)
   â†’ Define success metrics (primary, secondary)
   â†’ Set stopping criteria (sequential testing)

2. Monitoring Loop (LoopAgent)
   â†’ Daily: Check sample size progress
   â†’ Daily: Early stopping check (futility, superiority)
   â†’ Weekly: Interim analysis

3. Decision (LlmAgent)
   â†’ Statistical significance (p < 0.05)
   â†’ Practical significance (effect size)
   â†’ Recommendation: Ship A, Ship B, or Iterate
```

**Statistical Methods:**
```python
# Frequentist approach
def frequentist_test(control, treatment):
    t_stat, p_value = ttest_ind(control, treatment)
    effect_size = cohen_d(control, treatment)

    return {
        "significant": p_value < 0.05,
        "p_value": p_value,
        "effect_size": effect_size,
        "interpretation": "large effect" if abs(effect_size) > 0.8 else "medium"
    }

# Bayesian approach
def bayesian_test(control, treatment):
    prob_b_better = calculate_posterior(control, treatment)
    expected_loss = calculate_expected_loss(control, treatment)

    return {
        "prob_b_better": prob_b_better,
        "decision": "ship_b" if prob_b_better > 0.95 and expected_loss < 0.01 else "wait",
        "expected_lift": (mean(treatment) - mean(control)) / mean(control)
    }
```

---

### **Categoria 6: Product & UX**

#### **12. UserFeedbackAnalyzerAgent** ðŸ’¬

**PadrÃ£o ADK:** ParallelAgent (classification) â†’ LlmAgent (synthesis) â†’ CoordinatorAgent (routing)

```
FLUXO:
1. Parallel Classification (ParallelAgent)
   â†’ Sentiment analysis (positive/neutral/negative)
   â†’ Topic modeling (feature requests, bugs, praise)
   â†’ Urgency detection (critical, high, medium, low)
   â†’ Intent classification (question, complaint, suggestion)

2. Synthesis (LlmAgent)
   â†’ Group similar feedback
   â†’ Extract common themes
   â†’ Prioritize by frequency + impact

3. Routing (CoordinatorAgent)
   â†’ Bug â†’ Engineering
   â†’ Feature request â†’ Product
   â†’ Complaint â†’ Customer Success
   â†’ Question â†’ Support
```

**Output Example:**
```json
{
    "period": "2025-01",
    "total_feedback": 1247,
    "sentiment_breakdown": {
        "positive": 632,
        "neutral": 398,
        "negative": 217
    },
    "top_themes": [
        {
            "theme": "Mobile app performance",
            "count": 89,
            "sentiment": "negative",
            "urgency": "high",
            "example": "App trava ao enviar foto",
            "recommendation": "Priorizar otimizaÃ§Ã£o mobile"
        },
        {
            "theme": "Export feature request",
            "count": 67,
            "sentiment": "neutral",
            "urgency": "medium",
            "example": "Quero exportar relatÃ³rios em Excel",
            "recommendation": "Adicionar ao roadmap Q2"
        }
    ],
    "critical_issues": [
        {
            "issue": "Perda de dados ao salvar",
            "count": 12,
            "first_reported": "2025-01-15",
            "status": "escalated"
        }
    ]
}
```

---

#### **13. ProductRoadmapAgent** ðŸ—ºï¸

**PadrÃ£o ADK:** ParallelAgent (data gathering) â†’ LlmAgent (prioritization) â†’ ReflectiveAgent (strategic fit)

```
FLUXO:
1. Parallel Data Gathering (ParallelAgent)
   â†’ User feedback (UserFeedbackAnalyzerAgent)
   â†’ Competitor analysis (web scraping, G2, Capterra)
   â†’ Market trends (Google Trends, industry reports)
   â†’ Technical debt assessment

2. Prioritization (LlmAgent)
   â†’ RICE scoring (Reach, Impact, Confidence, Effort)
   â†’ Value vs Effort matrix
   â†’ Dependencies mapping

3. Strategic Fit (ReflectiveAgent)
   â†’ Alinhamento com visÃ£o de produto
   â†’ Strategic initiatives alignment
   â†’ Risk assessment
```

**RICE Scoring:**
```python
def calculate_rice_score(feature):
    reach = feature.users_impacted  # Monthly users affected
    impact = feature.impact_score  # 0.25=minimal, 0.5=low, 1=medium, 2=high, 3=massive
    confidence = feature.confidence  # 0.8 = 80% confident
    effort = feature.effort_person_months  # Person-months

    rice_score = (reach * impact * confidence) / effort

    return {
        "score": rice_score,
        "priority": "P0" if rice_score > 100 else "P1" if rice_score > 50 else "P2"
    }
```

---

### **Categoria 7: Customer Success**

#### **14. ChurnPredictionAgent** âš ï¸

**PadrÃ£o ADK:** ParallelAgent (feature engineering) â†’ LlmAgent (prediction) â†’ CoordinatorAgent (intervention)

```
FLUXO:
1. Feature Engineering (ParallelAgent)
   â†’ Engagement metrics (DAU, WAU, MAU)
   â†’ Feature usage (last used, frequency)
   â†’ Support tickets (count, sentiment)
   â†’ Payment history (delays, disputes)
   â†’ NPS score trend

2. Churn Prediction (LlmAgent)
   â†’ ML model (XGBoost, Random Forest)
   â†’ Churn probability (0-1)
   â†’ Contributing factors (SHAP values)

3. Intervention Routing (CoordinatorAgent)
   â†’ High risk (>80%) â†’ Urgent outreach (CEO)
   â†’ Medium risk (50-80%) â†’ Proactive CS check-in
   â†’ Low risk (<50%) â†’ Automated nurture campaign
```

**Churn Signals:**
```python
CHURN_SIGNALS = {
    "strong": [
        "login_days_since_last > 14",
        "feature_usage_decline > 50%",
        "support_ticket_negative_sentiment",
        "payment_delay > 7_days",
        "nps_score < 5"
    ],
    "medium": [
        "login_frequency_decline > 30%",
        "no_feature_adoption_last_30_days",
        "competitor_mentioned_in_tickets"
    ],
    "weak": [
        "nps_score_decline",
        "support_ticket_increase"
    ]
}
```

**Output:**
```json
{
    "contact_id": "contact-456",
    "company": "Company A",
    "churn_probability": 0.87,
    "risk_level": "high",
    "expected_churn_date": "2025-02-15",
    "contributing_factors": [
        {
            "factor": "login_days_since_last",
            "value": 21,
            "shap_value": 0.31,
            "importance": "high"
        },
        {
            "factor": "feature_usage_decline",
            "value": "62%",
            "shap_value": 0.24,
            "importance": "high"
        },
        {
            "factor": "support_negative_sentiment",
            "value": -0.45,
            "shap_value": 0.18,
            "importance": "medium"
        }
    ],
    "recommended_actions": [
        {
            "action": "executive_outreach",
            "owner": "CSM",
            "deadline": "2025-01-20",
            "template": "churn_risk_executive_call"
        },
        {
            "action": "feature_training",
            "owner": "Success Team",
            "deadline": "2025-01-22",
            "focus": "unused_premium_features"
        },
        {
            "action": "retention_offer",
            "owner": "Sales",
            "deadline": "2025-01-25",
            "max_discount": "30%"
        }
    ]
}
```

---

### **Categoria 8: Marketing Automation** ðŸ“£

#### **15. CampaignOrchestratorAgent** ðŸŽ¯

**PadrÃ£o ADK:** SequentialAgent (design) â†’ ParallelAgent (execution) â†’ LoopAgent (optimization)

**IntegraÃ§Ã£o Ventros:**
- Usa `Campaign` aggregate (multi-step campaigns)
- Cria/modifica `CampaignStep` conforme performance
- Monitora `Campaign.Stats` (contactsReached, conversionsCount)
- Gerencia `Sequence` enrollment based on behavior

```
FLUXO:
1. Campaign Design (LlmAgent)
   â†’ Analisa goal (reach_contacts, conversions, engagement)
   â†’ Sugere campaign structure baseado em best practices
   â†’ Define steps ideais (sequence, wait, conditional)
   â†’ Calcula audience size estimado

2. Parallel Execution (ParallelAgent)
   â†’ Sequence enrollment (trigger contact_list join)
   â†’ Broadcast dispatch (mass messaging)
   â†’ A/B test variants (split testing)
   â†’ Tracking UTM generation

3. Real-time Optimization (LoopAgent)
   â†’ Monitor campaign stats every hour
   â†’ IF conversion_rate < expected:
       â†’ Pause underperforming steps
       â†’ Reallocate budget to top performers
   â†’ IF goal reached:
       â†’ Complete campaign
   â†’ ELSE continue monitoring
```

**KnowledgeScope:**
```python
{
    "lookback_days": 90,
    "include_campaign_history": True,
    "include_contact_engagement": True,
    "include_market_benchmarks": True,
    "optimization_frequency": "hourly"
}
```

**MCP Tools Integration:**
```python
# Ventros-specific tools
tools=[
    Tool("create_campaign", func=create_campaign_via_go_api),
    Tool("add_campaign_step", func=add_step_to_campaign),
    Tool("get_campaign_stats", func=get_stats_from_campaign),
    Tool("enroll_in_sequence", func=enroll_contacts_in_sequence),
    Tool("send_broadcast", func=trigger_broadcast),
    Tool("get_contact_lists", func=mcp_client.get_contact_lists),
    Tool("create_tracking_link", func=create_utm_tracking)
]
```

**Use Cases:**
- "Crie uma campanha de Black Friday para lista X"
- "Otimize minha campanha de reativaÃ§Ã£o"
- "Por que minha campanha nÃ£o estÃ¡ convertendo?"

**Output Example:**
```json
{
    "campaign_id": "camp-2025-001",
    "name": "Black Friday 2025 - ReativaÃ§Ã£o",
    "status": "active",
    "designed_structure": {
        "step_1": {
            "type": "sequence",
            "sequence_name": "BF_Teaser",
            "trigger": "list_joined",
            "target_list": "dormant_customers_90d",
            "estimated_reach": 1247
        },
        "step_2": {
            "type": "wait",
            "duration_hours": 48
        },
        "step_3": {
            "type": "conditional",
            "condition": "email_opened OR link_clicked",
            "if_true": {
                "action": "send_broadcast",
                "template": "bf_offer_30_discount"
            },
            "if_false": {
                "action": "enroll_sequence",
                "sequence": "BF_Last_Chance"
            }
        }
    },
    "real_time_optimization": {
        "monitored_metrics": ["open_rate", "click_rate", "conversion_rate"],
        "optimization_actions": [
            {
                "hour": 12,
                "action": "paused_step_3_variant_b",
                "reason": "conversion_rate 2.1% vs 4.5% (variant A)",
                "impact": "reallocated 500 contacts to variant A"
            }
        ]
    },
    "current_stats": {
        "contacts_reached": 847,
        "conversions": 38,
        "conversion_rate": 4.48,
        "revenue_generated": 45600.00,
        "roi": 3.2
    }
}
```

---

#### **16. PersonalizationAgent** ðŸ’Ž

**PadrÃ£o ADK:** ParallelAgent (data enrichment) â†’ LlmAgent (content generation) â†’ ReflectiveAgent (quality check)

**IntegraÃ§Ã£o Ventros:**
- Usa `Contact` custom fields
- LÃª `Session` history e `Message` context
- Atualiza `Contact` tags dinamicamente
- Personaliza `Broadcast` e `Sequence` messages

```
FLUXO:
1. Data Enrichment (ParallelAgent)
   â†’ Contact profile (name, company, role, industry)
   â†’ Behavioral data (last_interaction, preferred_channel, engagement_score)
   â†’ Purchase history (LTV, last_purchase, avg_order_value)
   â†’ Real-time intent (current_page, cart_items, recent_searches)

2. Hyper-Personalization (LlmAgent)
   â†’ Generate 1-to-1 personalized message
   â†’ Adapt tone to contact profile
   â†’ Include dynamic product recommendations
   â†’ Personalize CTA based on funnel stage

3. Quality Check (ReflectiveAgent)
   â†’ Grammar and spelling
   â†’ Brand voice consistency
   â†’ Personalization accuracy (correct name, company, etc)
   â†’ CAN-SPAM / LGPD compliance
```

**Personalization Dimensions:**
```python
PERSONALIZATION_LAYERS = {
    "basic": [
        "first_name",
        "company_name",
        "industry"
    ],
    "behavioral": [
        "last_product_viewed",
        "cart_abandonment",
        "email_engagement_score",
        "preferred_content_type"
    ],
    "predictive": [
        "next_best_offer",
        "churn_risk",
        "upsell_opportunity",
        "optimal_send_time"
    ],
    "contextual": [
        "weather",  # Weather-based messaging
        "local_events",  # Event-triggered campaigns
        "time_zone",  # Send time optimization
        "device_type"  # Mobile vs desktop optimization
    ]
}
```

**Example - Before/After:**
```
BEFORE (Generic):
"Oi! Temos uma promoÃ§Ã£o especial para vocÃª. Aproveite 20% de desconto."

AFTER (Hyper-Personalized):
"Oi Leonardo! Vi que vocÃª estava olhando nosso plano Enterprise na terÃ§a-feira.

Como Head of Engineering na TechCorp, sei que escalabilidade Ã© prioridade.

Por isso, preparei uma proposta especial:
- Plano Enterprise com 30% de desconto (vs 20% padrÃ£o)
- Onboarding personalizado para seu time de 25 devs
- IntegraÃ§Ã£o com seu GitHub Enterprise (que vi que vocÃªs usam)

Melhor horÃ¡rio para conversar? Seus emails anteriores sugerem que vocÃª responde mais entre 14h-16h ðŸ˜Š

Leonardo Silva
Account Executive
"
```

---

#### **17. ABMAgent** (Account-Based Marketing) ðŸŽ–ï¸

**PadrÃ£o ADK:** CoordinatorAgent (account orchestration) â†’ ParallelAgent (multi-channel touch) â†’ LoopAgent (account nurturing)

**IntegraÃ§Ã£o Ventros:**
- Cria `Contact_list` dinÃ¢micas por account/company
- Orquestra `Campaign` multi-touch por account
- Usa `Pipeline` para track account progression
- Gera `Tracking` links por stakeholder

```
FLUXO:
1. Account Identification (LlmAgent)
   â†’ Segment contacts by company (firmographic data)
   â†’ Identify decision-makers vs influencers
   â†’ Map buying committee (CFO, CTO, CEO, etc)
   â†’ Score account fit (ICP match)

2. Multi-Channel Orchestration (ParallelAgent)
   â†’ Email sequence to C-level
   â†’ LinkedIn outreach to champions
   â†’ WhatsApp to decision-maker (if mobile available)
   â†’ Retargeting ads to account IP range

3. Account Nurturing Loop (LoopAgent)
   â†’ Track account engagement across all channels
   â†’ IF engagement_score > 70:
       â†’ Route to Sales (move to "qualified" pipeline stage)
   â†’ ELIF engagement_score 40-70:
       â†’ Escalate touch frequency (daily â†’ 3x/week)
   â†’ ELSE:
       â†’ Maintain nurture cadence (weekly)
```

**Account Scoring Model:**
```python
def calculate_account_score(account: dict) -> dict:
    # Fit Score (company matches ICP)
    fit_score = 0
    if account["employee_count"] >= 50: fit_score += 25
    if account["revenue"] >= 10_000_000: fit_score += 25
    if account["industry"] in TARGET_INDUSTRIES: fit_score += 25
    if account["tech_stack_match"] >= 0.7: fit_score += 25

    # Engagement Score (behavioral)
    engagement_score = 0
    engagement_score += account["website_visits"] * 2
    engagement_score += account["email_opens"] * 5
    engagement_score += account["content_downloads"] * 10
    engagement_score += account["demo_requests"] * 50

    # Intent Score (buying signals)
    intent_score = 0
    if "pricing" in account["recent_page_views"]: intent_score += 30
    if "case studies" in account["content_consumed"]: intent_score += 20
    if account["linkedin_engagement"]: intent_score += 15
    if account["competitor_comparison_viewed"]: intent_score += 35

    total_score = (fit_score + min(engagement_score, 100) + intent_score) / 3

    return {
        "overall_score": total_score,
        "fit_score": fit_score,
        "engagement_score": min(engagement_score, 100),
        "intent_score": intent_score,
        "tier": "tier_1" if total_score >= 75 else "tier_2" if total_score >= 50 else "tier_3"
    }
```

**Output:**
```json
{
    "account_id": "acc-techcorp-001",
    "company_name": "TechCorp Inc",
    "contacts": [
        {
            "name": "JoÃ£o Silva",
            "role": "CTO",
            "buyer_journey_stage": "awareness",
            "engagement_score": 45,
            "last_touch": "2025-01-10",
            "next_action": "LinkedIn connection request"
        },
        {
            "name": "Maria Santos",
            "role": "CFO",
            "buyer_journey_stage": "consideration",
            "engagement_score": 72,
            "last_touch": "2025-01-12",
            "next_action": "Send ROI calculator"
        }
    ],
    "account_score": {
        "overall": 68,
        "tier": "tier_2",
        "fit": 75,
        "engagement": 58,
        "intent": 45
    },
    "orchestration_plan": {
        "current_play": "enterprise_awareness_to_consideration",
        "touchpoints": [
            {"day": 1, "channel": "email", "target": "CTO", "template": "tech_innovation_story"},
            {"day": 3, "channel": "linkedin", "target": "CFO", "template": "roi_case_study"},
            {"day": 5, "channel": "whatsapp", "target": "CTO", "template": "demo_invitation"},
            {"day": 7, "channel": "email", "target": "both", "template": "webinar_invite"}
        ]
    }
}
```

---

#### **18. ContentGeneratorAgent** âœï¸

**PadrÃ£o ADK:** LlmAgent (content creation) â†’ ParallelAgent (multi-format adaptation) â†’ ReflectiveAgent (brand consistency)

**IntegraÃ§Ã£o Ventros:**
- Gera templates para `Sequence` steps
- Cria copy para `Broadcast` messages
- Adapta conteÃºdo para cada `Channel` (WhatsApp, Email, SMS)
- MantÃ©m brand voice consistency

```
FLUXO:
1. Content Strategy (LlmAgent)
   â†’ Analyze campaign goal
   â†’ Define content pillars
   â†’ Map funnel stage (TOFU, MOFU, BOFU)
   â†’ Choose content format (educational, promotional, social proof)

2. Multi-Format Generation (ParallelAgent)
   â†’ Email copy (subject + body + CTA)
   â†’ WhatsApp message (conversational, emoji-friendly)
   â†’ SMS (ultra-concise, link + CTA)
   â†’ Social media caption (Twitter, LinkedIn, Instagram)

3. Brand Consistency Check (ReflectiveAgent)
   â†’ Tone analysis (matches brand voice guidelines?)
   â†’ Terminology check (uses approved terms?)
   â†’ Compliance check (CAN-SPAM, LGPD disclaimers)
   â†’ A/B variant generation (create 2-3 variants)
```

**Content Templates por Funnel Stage:**
```python
CONTENT_FRAMEWORKS = {
    "TOFU": {  # Top of Funnel - Awareness
        "frameworks": ["PAS", "Before-After-Bridge", "Storytelling"],
        "tone": "educational, helpful, non-salesy",
        "cta": "Learn more, Download guide, Read article",
        "example": "ðŸŽ¯ VocÃª sabia que 70% das empresas perdem leads por falta de follow-up?..."
    },
    "MOFU": {  # Middle of Funnel - Consideration
        "frameworks": ["FAB", "Comparison", "Case Study"],
        "tone": "consultative, value-focused",
        "cta": "See how it works, Book a demo, Calculate ROI",
        "example": "Como a TechCorp aumentou conversÃ£o em 40% com nossa plataforma..."
    },
    "BOFU": {  # Bottom of Funnel - Decision
        "frameworks": ["Urgency + Scarcity", "Social Proof", "Risk Reversal"],
        "tone": "confident, direct, action-oriented",
        "cta": "Start free trial, Schedule implementation, Get quote",
        "example": "â° Ãšltimas 48h: 30% de desconto + onboarding grÃ¡tis. Apenas para..."
    }
}
```

**Example Output:**
```json
{
    "campaign": "Q1_Lead_Nurturing",
    "funnel_stage": "MOFU",
    "variants_generated": [
        {
            "variant": "A",
            "channels": {
                "email": {
                    "subject": "Como a TechCorp dobrou conversÃµes em 30 dias",
                    "preheader": "Case study completo + template gratuito",
                    "body": "Oi {{first_name}},\n\nVi que vocÃª baixou nosso guia...",
                    "cta": "Ver case study completo",
                    "estimated_length": "~150 palavras"
                },
                "whatsapp": {
                    "message": "Oi {{first_name}}! ðŸ‘‹\n\nLembra do guia que vocÃª baixou semana passada sobre lead nurturing?\n\nAcabei de publicar um case study da TechCorp que conseguiu DOBRAR as conversÃµes em 30 dias...",
                    "cta_button": "ðŸ“Š Ver resultados",
                    "estimated_length": "~60 palavras"
                },
                "sms": {
                    "message": "{{first_name}}, case TechCorp: +100% conversÃµes em 30d. Veja como: {{short_link}}",
                    "estimated_length": "~120 chars"
                }
            }
        },
        {
            "variant": "B",
            "channels": {
                "email": {
                    "subject": "{{first_name}}, vocÃª estÃ¡ perdendo 50% das suas conversÃµes?",
                    "preheader": "Descubra o erro que 73% das empresas cometem",
                    "body": "A maioria das empresas perde metade dos leads qualificados...",
                    "cta": "Descobrir o erro",
                    "estimated_length": "~140 palavras"
                }
            }
        }
    ],
    "brand_compliance_check": {
        "tone_match": 0.92,
        "terminology_approved": true,
        "lgpd_compliant": true,
        "warnings": []
    },
    "recommendation": "variant_a",
    "reasoning": "Case study approach aligns better with consideration stage"
}
```

---

#### **19. FunnelOptimizationAgent** ðŸ”§

**PadrÃ£o ADK:** ParallelAgent (data analysis) â†’ LlmAgent (bottleneck detection) â†’ CoordinatorAgent (fix orchestration)

**IntegraÃ§Ã£o Ventros:**
- Analisa `Pipeline` stages e conversion rates
- Identifica drop-offs em `Sequence` steps
- Otimiza `Campaign` step ordering
- Recomenda `Contact_list` segmentation changes

```
FLUXO:
1. Funnel Analysis (ParallelAgent)
   â†’ Pipeline stage conversion rates
   â†’ Sequence step completion rates
   â†’ Campaign goal achievement rates
   â†’ Time-to-conversion by segment

2. Bottleneck Detection (LlmAgent)
   â†’ Identify lowest-converting steps
   â†’ Analyze drop-off reasons (timing, messaging, audience fit)
   â†’ Compare to industry benchmarks
   â†’ Root cause analysis

3. Fix Orchestration (CoordinatorAgent)
   â†’ IF bottleneck = "poor messaging":
       â†’ Route to ContentGeneratorAgent for rewrite
   â†’ IF bottleneck = "wrong timing":
       â†’ Adjust sequence delays
   â†’ IF bottleneck = "audience mismatch":
       â†’ Refine contact_list filters
```

**Funnel Metrics Tracked:**
```python
FUNNEL_METRICS = {
    "pipeline_stages": {
        "lead": {
            "contacts": 1000,
            "conversion_to_next": 0.45,  # 45% convert to qualified
            "avg_time_in_stage_days": 7,
            "benchmark": 0.40  # Industry benchmark
        },
        "qualified": {
            "contacts": 450,
            "conversion_to_next": 0.30,  # 30% convert to opportunity
            "avg_time_in_stage_days": 14,
            "benchmark": 0.35  # BELOW benchmark âš ï¸
        },
        "opportunity": {
            "contacts": 135,
            "conversion_to_next": 0.60,  # 60% convert to customer
            "avg_time_in_stage_days": 21,
            "benchmark": 0.55
        }
    },
    "sequence_performance": {
        "Lead_Nurturing_Sequence": {
            "enrolled": 1000,
            "completed": 320,
            "exited_early": 680,
            "steps": [
                {"step": 1, "completion_rate": 0.92},
                {"step": 2, "completion_rate": 0.78},
                {"step": 3, "completion_rate": 0.45},  # âš ï¸ Big drop
                {"step": 4, "completion_rate": 0.40},
                {"step": 5, "completion_rate": 0.32}
            ]
        }
    }
}
```

**Optimization Recommendations:**
```json
{
    "funnel_analysis": {
        "overall_conversion_rate": 0.081,  # 8.1% (lead â†’ customer)
        "industry_benchmark": 0.095,  # 9.5%
        "gap": -1.4,
        "status": "underperforming"
    },
    "bottlenecks_detected": [
        {
            "stage": "qualified",
            "issue": "Below-benchmark conversion to opportunity",
            "current": 0.30,
            "benchmark": 0.35,
            "impact": "14% revenue loss ($42K/month)",
            "root_cause": "Sales follow-up delays (avg 3.2 days vs 1.5 benchmark)",
            "fix_priority": "P0"
        },
        {
            "sequence": "Lead_Nurturing_Sequence",
            "step": 3,
            "issue": "43% drop-off at step 3",
            "current": 0.45,
            "expected": 0.75,
            "root_cause": "Generic product pitch (not personalized)",
            "fix_priority": "P1"
        }
    ],
    "recommended_actions": [
        {
            "action": "implement_sla_qualified_followup",
            "description": "Auto-assign qualified leads to sales within 1h",
            "expected_impact": "+5% conversion (qualified â†’ opportunity)",
            "implementation": "Add automation: Pipeline.qualified â†’ Assign.agent",
            "effort": "low",
            "roi": "high"
        },
        {
            "action": "personalize_sequence_step_3",
            "description": "Use PersonalizationAgent to rewrite step 3 with behavior-based personalization",
            "expected_impact": "+15% step completion",
            "implementation": "Route to ContentGeneratorAgent with contact behavioral data",
            "effort": "medium",
            "roi": "high"
        },
        {
            "action": "ab_test_sequence_timing",
            "description": "Test sending step 3 after 2 days vs 4 days",
            "expected_impact": "TBD (requires testing)",
            "implementation": "Create AB test with 50/50 split",
            "effort": "low",
            "roi": "medium"
        }
    ]
}
```

---

### **Categoria 9: Sales Enablement** ðŸ’¼

#### **20. LeadScoringAgent** ðŸŽ¯

**PadrÃ£o ADK:** ParallelAgent (data collection) â†’ LlmAgent (scoring) â†’ CoordinatorAgent (routing)

**IntegraÃ§Ã£o Ventros:**
- Atualiza `Contact` custom field: `lead_score`
- Move contacts entre `Pipeline` stages based on score
- Prioriza `Session` routing to best agent
- Trigger `Sequence` enrollment based on score threshold

```
FLUXO:
1. Data Collection (ParallelAgent)
   â†’ Demographic data (company size, industry, role, location)
   â†’ Behavioral data (website visits, email engagement, content consumed)
   â†’ Firmographic data (revenue, growth rate, tech stack)
   â†’ Intent signals (pricing page views, competitor comparisons, demo requests)

2. AI Scoring (LlmAgent)
   â†’ Traditional lead scoring (points-based)
   â†’ Predictive lead scoring (ML model: XGBoost, LightGBM)
   â†’ Lookalike modeling (similar to best customers)
   â†’ Intent decay (recent signals weighted higher)

3. Intelligent Routing (CoordinatorAgent)
   â†’ Hot leads (score >80) â†’ Senior AE, immediate follow-up
   â†’ Warm leads (score 50-80) â†’ Standard AE, 24h SLA
   â†’ Cold leads (score <50) â†’ Nurture sequence, no manual touch
```

**Scoring Model:**
```python
# Predictive Lead Scoring with ML
class LeadScoringModel:
    def __init__(self):
        self.model = load_pretrained_model("xgboost_lead_scoring_v2.pkl")

    def score_lead(self, contact: dict) -> dict:
        # Feature engineering
        features = {
            # Demographic (25%)
            "company_size_score": self.normalize_company_size(contact["employee_count"]),
            "industry_fit": 1 if contact["industry"] in TARGET_INDUSTRIES else 0,
            "role_seniority": self.map_role_seniority(contact["role"]),

            # Firmographic (25%)
            "revenue_band": self.normalize_revenue(contact["company_revenue"]),
            "growth_rate": contact.get("yoy_growth", 0),
            "tech_stack_match": self.calculate_tech_match(contact["technologies"]),

            # Behavioral (30%)
            "website_visits_30d": contact["website_visits"],
            "email_engagement_score": contact["email_open_rate"] * 0.3 + contact["click_rate"] * 0.7,
            "content_consumption_score": len(contact["content_downloaded"]) * 5,
            "session_count": contact["total_sessions"],
            "avg_session_duration": contact["avg_session_duration_seconds"],

            # Intent (20%)
            "pricing_page_views": contact.get("pricing_views", 0),
            "demo_requests": contact.get("demo_requests", 0),
            "competitor_comparison": 1 if "vs-competitor" in contact["recent_pages"] else 0,
            "high_intent_pages": self.count_high_intent_pages(contact["page_history"]),
            "recency_days": (datetime.now() - contact["last_activity"]).days
        }

        # ML prediction
        score = self.model.predict_proba([list(features.values())])[0][1] * 100

        # Decay factor (recent activity weighted higher)
        decay_factor = 1 - (features["recency_days"] / 90)  # 90-day decay
        adjusted_score = score * max(decay_factor, 0.3)  # Min 30% of original score

        return {
            "lead_score": round(adjusted_score, 2),
            "grade": self.assign_grade(adjusted_score),
            "conversion_probability": round(score / 100, 3),
            "feature_importance": self.get_top_features(features),
            "recommended_action": self.recommend_action(adjusted_score)
        }

    def assign_grade(self, score: float) -> str:
        if score >= 80: return "A"  # Hot
        if score >= 60: return "B"  # Warm
        if score >= 40: return "C"  # Lukewarm
        return "D"  # Cold

    def recommend_action(self, score: float) -> str:
        if score >= 80: return "immediate_sales_call"
        if score >= 60: return "personalized_email_sequence"
        if score >= 40: return "nurture_sequence"
        return "content_drip_campaign"
```

**Output:**
```json
{
    "contact_id": "contact-456",
    "lead_score": 78,
    "grade": "B",
    "conversion_probability": 0.82,
    "segment": "warm_lead",
    "top_positive_signals": [
        {"signal": "3x pricing page visits (last 7 days)", "impact": "+18 points"},
        {"signal": "Downloaded ROI calculator", "impact": "+15 points"},
        {"signal": "CTO role (decision-maker)", "impact": "+12 points"},
        {"signal": "Company size 250+ employees (ICP match)", "impact": "+10 points"}
    ],
    "top_negative_signals": [
        {"signal": "Last activity 9 days ago", "impact": "-8 points"},
        {"signal": "Email open rate 12% (below avg)", "impact": "-5 points"}
    ],
    "recommended_action": {
        "action": "personalized_outreach_within_24h",
        "assignee": "Senior AE",
        "template": "high_intent_pricing_inquiry",
        "sla": "24 hours"
    },
    "lookalike_customers": [
        {"customer": "TechCorp", "similarity": 0.89, "ltv": 125000},
        {"customer": "InnovateCo", "similarity": 0.84, "ltv": 98000}
    ]
}
```

---

#### **21. SalesForecastingAgent** ðŸ“ˆ

**PadrÃ£o ADK:** ParallelAgent (data aggregation) â†’ LlmAgent (prediction) â†’ ReflectiveAgent (sanity check)

**IntegraÃ§Ã£o Ventros:**
- Analisa `Pipeline` opportunities e deal values
- PrevÃª `Campaign` conversion outcomes
- Calcula `Contact` lifetime value prediction
- Alerta sobre deal risks via `Session` context

```
FLUXO:
1. Data Aggregation (ParallelAgent)
   â†’ Open opportunities by stage
   â†’ Historical close rates by rep, stage, segment
   â†’ Deal velocity (avg days in each stage)
   â†’ Seasonal patterns (Q4 surge, summer slowdown)

2. AI Forecasting (LlmAgent)
   â†’ Time series forecasting (ARIMA, Prophet)
   â†’ Deal-level probability scoring
   â†’ Pipeline coverage analysis (3x rule check)
   â†’ Risk-adjusted forecast

3. Sanity Check (ReflectiveAgent)
   â†’ Compare to last quarter
   â†’ Flag unrealistic projections
   â†’ Identify data quality issues
   â†’ Provide confidence intervals
```

**Forecasting Methods:**
```python
class SalesForecastingEngine:
    def __init__(self):
        self.historical_data = load_historical_sales()
        self.prophet_model = Prophet()

    def generate_forecast(self, forecast_period: str = "Q1_2025") -> dict:
        # Method 1: Historical trend (baseline)
        historical_forecast = self.calculate_historical_trend()

        # Method 2: Pipeline-based (weighted by stage)
        pipeline_forecast = self.calculate_pipeline_weighted()

        # Method 3: AI predictive (Prophet + features)
        ai_forecast = self.calculate_ai_forecast()

        # Ensemble (weighted average)
        final_forecast = (
            historical_forecast * 0.2 +
            pipeline_forecast * 0.5 +  # Highest weight to pipeline
            ai_forecast * 0.3
        )

        return {
            "period": forecast_period,
            "forecast": final_forecast,
            "confidence_interval": {
                "low": final_forecast * 0.85,
                "high": final_forecast * 1.15
            },
            "methods": {
                "historical_trend": historical_forecast,
                "pipeline_weighted": pipeline_forecast,
                "ai_predictive": ai_forecast
            },
            "pipeline_coverage": self.calculate_pipeline_coverage(),
            "at_risk_deals": self.identify_at_risk_deals()
        }

    def calculate_pipeline_weighted(self) -> float:
        """Stage-weighted pipeline forecast"""
        stage_probabilities = {
            "lead": 0.05,
            "qualified": 0.15,
            "opportunity": 0.35,
            "proposal": 0.60,
            "negotiation": 0.80,
            "closed_won": 1.0
        }

        forecast = 0
        for deal in self.get_open_deals():
            stage_prob = stage_probabilities.get(deal["stage"], 0.2)
            ai_prob = self.calculate_deal_probability(deal)
            combined_prob = (stage_prob + ai_prob) / 2
            forecast += deal["value"] * combined_prob

        return forecast

    def identify_at_risk_deals(self) -> list:
        """Identify deals at risk of slipping"""
        at_risk = []
        for deal in self.get_open_deals():
            risk_signals = []

            # Stalled in stage
            if deal["days_in_current_stage"] > deal["avg_days_in_stage"] * 1.5:
                risk_signals.append("stalled_in_stage")

            # Low engagement
            if deal["last_activity_days"] > 7:
                risk_signals.append("low_engagement")

            # Missing next steps
            if not deal["next_steps"]:
                risk_signals.append("no_next_steps")

            # Competitor mention
            if "competitor" in deal["recent_notes"].lower():
                risk_signals.append("competitive_threat")

            if risk_signals:
                at_risk.append({
                    "deal_id": deal["id"],
                    "value": deal["value"],
                    "risk_signals": risk_signals,
                    "probability_to_close": self.calculate_deal_probability(deal),
                    "recommended_action": "urgent_follow_up"
                })

        return at_risk
```

**Output:**
```json
{
    "forecast_period": "Q1_2025",
    "forecast_value": 1_250_000,
    "confidence": "medium",
    "confidence_interval": {
        "low": 1_062_500,
        "high": 1_437_500,
        "confidence_level": 0.80
    },
    "forecast_breakdown": {
        "historical_trend": 1_180_000,
        "pipeline_weighted": 1_320_000,
        "ai_predictive": 1_245_000
    },
    "pipeline_analysis": {
        "total_pipeline_value": 3_750_000,
        "weighted_pipeline": 1_320_000,
        "coverage_ratio": 3.0,  # 3x target (healthy)
        "deals_by_stage": {
            "qualified": {"count": 45, "value": 750_000, "weighted": 112_500},
            "opportunity": {"count": 32, "value": 1_200_000, "weighted": 420_000},
            "proposal": {"count": 18, "value": 900_000, "weighted": 540_000},
            "negotiation": {"count": 8, "value": 900_000, "weighted": 720_000}
        }
    },
    "at_risk_deals": [
        {
            "deal_id": "deal-123",
            "company": "TechCorp",
            "value": 85_000,
            "stage": "proposal",
            "risk_level": "high",
            "risk_signals": [
                "stalled_30_days",
                "no_activity_14_days",
                "competitor_mentioned"
            ],
            "probability_to_close": 0.35,  # Down from 0.60
            "recommended_action": "executive_escalation",
            "action_owner": "VP Sales"
        }
    ],
    "insights": [
        "Pipeline coverage is healthy at 3x target",
        "âš ï¸ 8 high-value deals ($450K) at risk of slipping to Q2",
        "Proposal â†’ Negotiation conversion is 12% below target",
        "Q4 momentum is strong, but Jan typically sees 20% slowdown"
    ]
}
```

---

#### **22. DealAssistantAgent** ðŸ¤

**PadrÃ£o ADK:** LlmAgent (deal analysis) â†’ ParallelAgent (research) â†’ CoordinatorAgent (playbook selection)

**IntegraÃ§Ã£o Ventros:**
- Analisa `Contact` em deal committee
- LÃª `Session` history com decision-makers
- Sugere `Message` next steps
- Atualiza `Pipeline` custom fields com insights

```
FLUXO:
1. Deal Analysis (LlmAgent)
   â†’ Parse deal notes, emails, call transcripts
   â†’ Identify pain points, objections, champions
   â†’ Map buying committee (roles, influence levels)
   â†’ Assess deal health (green/yellow/red)

2. Competitive Intelligence (ParallelAgent)
   â†’ Research competitor mentioned
   â†’ Find competitive battlecards
   â†’ Analyze win/loss patterns vs this competitor
   â†’ Suggest differentiation talking points

3. Playbook Selection (CoordinatorAgent)
   â†’ IF deal_health = "red":
       â†’ Route to "save_deal_playbook"
   â†’ IF objection = "price":
       â†’ Route to "roi_justification_playbook"
   â†’ IF multi_stakeholder:
       â†’ Route to "consensus_building_playbook"
```

**Deal Health Scoring:**
```python
def assess_deal_health(deal: dict) -> dict:
    health_score = 100  # Start at 100, deduct for red flags

    # Red flags
    red_flags = []

    # Engagement red flags
    if deal["days_since_last_activity"] > 7:
        health_score -= 20
        red_flags.append("no_recent_activity")

    if deal["champion_engaged"] == False:
        health_score -= 15
        red_flags.append("champion_not_engaged")

    # Budget red flags
    if deal.get("budget_confirmed") == False:
        health_score -= 10
        red_flags.append("budget_not_confirmed")

    # Timeline red flags
    if deal["days_in_stage"] > deal["avg_days_in_stage"] * 2:
        health_score -= 15
        red_flags.append("deal_stalled")

    # Decision process red flags
    if not deal.get("decision_criteria"):
        health_score -= 10
        red_flags.append("unclear_decision_criteria")

    if deal.get("competitors_involved", 0) > 1:
        health_score -= 10
        red_flags.append("multiple_competitors")

    # Green flags
    green_flags = []

    if deal.get("executive_sponsor_engaged"):
        health_score += 10
        green_flags.append("executive_sponsor")

    if deal.get("trial_active"):
        health_score += 5
        green_flags.append("active_trial")

    if deal.get("contract_sent"):
        health_score += 10
        green_flags.append("contract_stage")

    # Assign grade
    if health_score >= 80:
        grade = "green"
    elif health_score >= 60:
        grade = "yellow"
    else:
        grade = "red"

    return {
        "health_score": max(health_score, 0),
        "grade": grade,
        "red_flags": red_flags,
        "green_flags": green_flags,
        "recommended_action": get_action_for_grade(grade, red_flags)
    }
```

**Output:**
```json
{
    "deal_id": "deal-456",
    "company": "InnovateCo",
    "value": 95_000,
    "stage": "negotiation",
    "health_assessment": {
        "grade": "yellow",
        "score": 68,
        "red_flags": [
            "no_activity_9_days",
            "budget_not_fully_confirmed",
            "competitor_salesforce_mentioned"
        ],
        "green_flags": [
            "executive_sponsor_engaged",
            "trial_active"
        ]
    },
    "buying_committee": [
        {
            "name": "Carlos Oliveira",
            "role": "CTO",
            "influence": "decision_maker",
            "sentiment": "champion",
            "last_interaction": "2025-01-08",
            "key_pain_point": "Legacy system integration complexity"
        },
        {
            "name": "Ana Costa",
            "role": "CFO",
            "influence": "decision_maker",
            "sentiment": "neutral",
            "last_interaction": "2024-12-20",
            "key_concern": "ROI timeline unclear"
        }
    ],
    "competitive_analysis": {
        "competitor": "Salesforce",
        "our_win_rate_vs_competitor": 0.65,
        "differentiation_points": [
            "50% lower TCO over 3 years",
            "Native WhatsApp integration (Salesforce requires 3rd party)",
            "Brazilian market expertise and LGPD compliance"
        ],
        "vulnerability": "Salesforce brand recognition stronger"
    },
    "recommended_actions": [
        {
            "action": "reengage_cfo_ana",
            "priority": "P0",
            "description": "Send personalized ROI analysis showing 8-month payback",
            "template": "roi_justification_cfo",
            "deadline": "2025-01-18"
        },
        {
            "action": "schedule_executive_briefing",
            "priority": "P1",
            "description": "CEO-to-CEO call to reinforce commitment",
            "participants": ["Our CEO", "InnovateCo CEO"],
            "deadline": "2025-01-22"
        },
        {
            "action": "send_salesforce_comparison",
            "priority": "P1",
            "description": "Battlecard focused on TCO and WhatsApp native integration",
            "template": "competitive_battlecard_salesforce",
            "deadline": "2025-01-20"
        }
    ],
    "playbook_triggered": "multi_stakeholder_alignment",
    "ai_insights": [
        "CFO Ana has not been engaged for 19 days - risk of losing internal champion",
        "CTO Carlos is actively using trial (15 sessions last week) - strong buy signal",
        "Deal velocity has slowed 40% - typical pattern when budget approval pending",
        "Similar deals with CTO+CFO combo close 20% faster with joint ROI session"
    ]
}
```

---

## âš¡ PODER DOS CUSTOM AGENTS (SEM LLM)

### **Por que usar Custom Agents (BaseAgent)?**

O ADK permite criar **Custom Agents** herdando de `BaseAgent` para **executar cÃ³digo Python diretamente sem usar LLMs**. Isso Ã© **revolucionÃ¡rio** para:

âœ… **Performance**: 100-1000x mais rÃ¡pido que LLM calls
âœ… **Custo**: Quase zero vs $0.01-0.10 por LLM call
âœ… **Determinismo**: Resultados consistentes e reproduzÃ­veis
âœ… **Offline**: Funciona sem internet/API externa
âœ… **PrecisÃ£o**: CÃ¡lculos matemÃ¡ticos/estatÃ­sticos exatos

### **Quando usar Custom Agents vs LLM Agents?**

| Tarefa | Use Custom Agent (BaseAgent) | Use LLM Agent |
|--------|------------------------------|---------------|
| **ML Model Inference** | âœ… XGBoost, Random Forest, Prophet | âŒ |
| **CÃ¡lculos MatemÃ¡ticos** | âœ… EstatÃ­stica, otimizaÃ§Ã£o, Ã¡lgebra | âŒ |
| **Processamento de Dados** | âœ… ETL, agregaÃ§Ãµes, transformaÃ§Ãµes | âŒ |
| **IntegraÃ§Ã£o com APIs** | âœ… Chamadas REST/gRPC diretas | âŒ |
| **GeraÃ§Ã£o de Texto** | âŒ | âœ… Email copy, relatÃ³rios |
| **RaciocÃ­nio Complexo** | âŒ | âœ… Planejamento, estratÃ©gia |
| **Natural Language Understanding** | âŒ | âœ… Intent detection, sentiment |

### **Pattern: Hybrid Agent (Custom + LLM)**

O padrÃ£o mais poderoso Ã© **combinar** Custom Agents para cÃ¡lculos e LLM Agents para raciocÃ­nio:

```python
# Exemplo: Lead Scoring HÃ­brido
LeadScoringAgent = SequentialAgent(
    agents=[
        # 1. Custom Agent: Coleta dados (RÃPIDO, sem LLM)
        DataCollectionAgent(),  # Herda de BaseAgent

        # 2. Custom Agent: ML inference (RÃPIDO, sem LLM)
        MLScoringAgent(),  # Herda de BaseAgent

        # 3. LlmAgent: Explica o score (USA LLM para gerar texto)
        ExplanationAgent(LlmAgent),

        # 4. Custom Agent: Roteia baseado no score (RÃPIDO, sem LLM)
        RoutingAgent()  # Herda de BaseAgent
    ]
)
```

### **ImplementaÃ§Ã£o: Custom Agent para Lead Scoring**

```python
from adk import BaseAgent, SequentialAgent, LlmAgent
import xgboost as xgb
import numpy as np
from datetime import datetime

class MLLeadScoringAgent(BaseAgent):
    """
    Custom Agent que roda XGBoost diretamente SEM LLM

    Performance: ~5ms por lead (vs 2000ms com LLM)
    Custo: $0 (vs $0.03 com LLM)
    """

    def __init__(self):
        super().__init__(name="ml_lead_scorer")

        # Load pre-trained XGBoost model
        self.model = xgb.Booster()
        self.model.load_model("models/lead_scoring_v2.json")

        # Feature engineering config
        self.target_industries = ["technology", "finance", "healthcare"]
        self.high_intent_pages = ["/pricing", "/demo", "/vs-competitor"]

    async def run_async(self, user_input, session):
        """
        Pure Python method - NO LLM calls!

        Args:
            user_input: Contact data from Ventros CRM
            session: ADK session with state

        Returns:
            AgentResponse with lead score + grade + routing
        """

        # Parse contact from user_input
        contact = user_input if isinstance(user_input, dict) else session.state.get("contact")

        # 1. Feature Engineering (deterministic)
        features = self._engineer_features(contact)

        # 2. ML Inference (XGBoost - NO LLM!)
        X = np.array([list(features.values())])
        dmatrix = xgb.DMatrix(X)
        score_raw = self.model.predict(dmatrix)[0] * 100

        # 3. Apply decay factor
        recency_days = features["recency_days"]
        decay = max(1 - (recency_days / 90), 0.3)
        score_final = score_raw * decay

        # 4. Grade assignment (deterministic)
        grade = self._assign_grade(score_final)

        # 5. Routing decision (deterministic)
        routing = self._decide_routing(score_final, grade)

        # 6. Return AgentResponse (ADK standard)
        result = {
            "lead_score": round(score_final, 2),
            "grade": grade,
            "conversion_probability": round(score_raw / 100, 3),
            "routing_decision": routing,
            "execution_time_ms": 5,  # FAST!
            "cost": 0.0  # FREE!
        }

        # Store in session for next agents
        session.state["lead_score_result"] = result

        return self.create_response(
            content=f"Lead scored: {grade} ({round(score_final, 2)})",
            metadata=result
        )

    def _engineer_features(self, contact: dict) -> dict:
        """Feature engineering - pure Python logic"""
        return {
            # Demographic (25%)
            "company_size_score": self._normalize_company_size(
                contact.get("employee_count", 0)
            ),
            "industry_fit": 1 if contact.get("industry") in self.target_industries else 0,
            "role_seniority": self._map_seniority(contact.get("role", "")),

            # Firmographic (25%)
            "revenue_band": self._normalize_revenue(contact.get("company_revenue", 0)),
            "growth_rate": contact.get("yoy_growth", 0),
            "tech_stack_match": self._calculate_tech_match(contact.get("technologies", [])),

            # Behavioral (30%)
            "website_visits_30d": contact.get("website_visits", 0),
            "email_open_rate": contact.get("email_open_rate", 0),
            "click_rate": contact.get("click_rate", 0),
            "content_downloads": len(contact.get("content_downloaded", [])),
            "session_count": contact.get("total_sessions", 0),
            "avg_session_duration": contact.get("avg_session_duration_seconds", 0),

            # Intent (20%)
            "pricing_views": contact.get("pricing_views", 0),
            "demo_requests": contact.get("demo_requests", 0),
            "competitor_comparison": 1 if any(
                page in contact.get("recent_pages", [])
                for page in self.high_intent_pages
            ) else 0,
            "recency_days": (datetime.now() - contact.get("last_activity")).days
        }

    def _assign_grade(self, score: float) -> str:
        """Deterministic grade assignment"""
        if score >= 80: return "A"
        if score >= 60: return "B"
        if score >= 40: return "C"
        return "D"

    def _decide_routing(self, score: float, grade: str) -> dict:
        """Deterministic routing logic"""
        if grade == "A":
            return {
                "assignee": "senior_ae",
                "sla_hours": 1,
                "priority": "urgent",
                "action": "immediate_sales_call"
            }
        elif grade == "B":
            return {
                "assignee": "standard_ae",
                "sla_hours": 24,
                "priority": "high",
                "action": "personalized_email_sequence"
            }
        elif grade == "C":
            return {
                "assignee": "automation",
                "sla_hours": None,
                "priority": "medium",
                "action": "nurture_sequence"
            }
        else:  # D
            return {
                "assignee": "automation",
                "sla_hours": None,
                "priority": "low",
                "action": "content_drip_campaign"
            }


# Usage: Hybrid Agent (Code + LLM)
class HybridLeadScoringAgent(SequentialAgent):
    """
    Hybrid: Fast ML scoring (Code) + Human explanation (LLM)

    Performance: ~2005ms total
    - 5ms for ML scoring (CodeAgent)
    - 2000ms for explanation (LlmAgent)

    Cost: $0.01 total
    - $0.00 for scoring
    - $0.01 for explanation
    """

    def __init__(self):
        super().__init__(
            name="hybrid_lead_scorer",
            agents=[
                # Step 1: FAST ML scoring (NO LLM)
                MLLeadScoringAgent(),

                # Step 2: Human-readable explanation (USES LLM)
                LlmAgent(
                    name="score_explainer",
                    model=GenerativeModel("gemini-2.0-flash-exp"),
                    instruction="""
                    Explique o lead score de forma clara para o time de vendas.

                    Entrada: score + features

                    SaÃ­da: ExplicaÃ§Ã£o em portuguÃªs, destacando:
                    1. Por que o score Ã© alto/mÃ©dio/baixo
                    2. Top 3 sinais positivos
                    3. Top 3 pontos de atenÃ§Ã£o
                    4. AÃ§Ã£o recomendada
                    """
                )
            ]
        )


# Example usage
async def score_lead_example():
    """Example: Score a lead using hybrid agent"""

    # Initialize hybrid agent
    agent = HybridLeadScoringAgent()

    # Contact data from Ventros CRM
    contact = {
        "id": "contact-456",
        "name": "JoÃ£o Silva",
        "company": "TechCorp",
        "industry": "technology",
        "employee_count": 250,
        "company_revenue": 15_000_000,
        "role": "CTO",
        "website_visits": 12,
        "email_open_rate": 0.45,
        "click_rate": 0.28,
        "pricing_views": 3,
        "demo_requests": 1,
        "last_activity": datetime(2025, 1, 10)
    }

    # Execute hybrid agent
    result = await agent.run_async(contact)

    print(result)
    # Output:
    # {
    #   "lead_score": 78.5,
    #   "grade": "B",
    #   "conversion_probability": 0.82,
    #   "routing_decision": {
    #     "assignee": "standard_ae",
    #     "sla_hours": 24,
    #     "action": "personalized_email_sequence"
    #   },
    #   "explanation": "JoÃ£o da TechCorp Ã© um lead WARM (grade B)...",
    #   "execution_time_ms": 2005,
    #   "cost_usd": 0.01
    # }
```

### **ImplementaÃ§Ã£o: Custom Agent para Sales Forecasting**

```python
from adk import BaseAgent, SequentialAgent
from prophet import Prophet
import pandas as pd
import numpy as np

class ProphetForecastingAgent(BaseAgent):
    """
    Custom Agent que roda Prophet para forecasting SEM LLM

    Performance: ~500ms (vs 3000ms com LLM)
    Custo: $0 (vs $0.05 com LLM)
    """

    def __init__(self):
        super().__init__(name="prophet_forecaster")

    async def run_async(self, user_input, session):
        """
        Time series forecasting usando Prophet - NO LLM!

        Args:
            user_input: {historical_data, forecast_days}
            session: ADK session

        Returns:
            AgentResponse with forecast + confidence intervals
        """

        # Parse input
        historical_data = user_input.get("historical_data", session.state.get("historical_data"))
        forecast_days = user_input.get("forecast_days", 90)

        # 1. Prepare data for Prophet
        df = pd.DataFrame(historical_data)
        df.columns = ["ds", "y"]  # Prophet requires these column names
        df["ds"] = pd.to_datetime(df["ds"])

        # 2. Initialize and fit Prophet model
        model = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=True,
            daily_seasonality=False,
            changepoint_prior_scale=0.05
        )
        model.fit(df)

        # 3. Create future dataframe
        future = model.make_future_dataframe(periods=forecast_days)

        # 4. Generate forecast
        forecast = model.predict(future)

        # 5. Extract relevant periods
        q1_forecast = forecast[forecast["ds"].dt.quarter == 1]

        result = {
            "forecast_period": "Q1_2025",
            "forecast_value": q1_forecast["yhat"].sum(),
            "confidence_interval": {
                "low": q1_forecast["yhat_lower"].sum(),
                "high": q1_forecast["yhat_upper"].sum()
            },
            "daily_forecast": q1_forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]].to_dict("records"),
            "execution_time_ms": 500,
            "cost": 0.0
        }

        # Store in session
        session.state["prophet_forecast"] = result

        return self.create_response(
            content=f"Q1 forecast: ${result['forecast_value']:,.0f}",
            metadata=result
        )


# Usage: Pure Custom Agents (NO LLM at all!)
class PureForecastingAgent(SequentialAgent):
    """
    Pure code-based forecasting - NO LLM CALLS!

    Performance: ~1000ms total (all code execution)
    Cost: $0 (completely free!)
    """

    def __init__(self):
        super().__init__(
            name="pure_forecasting",
            agents=[
                # Step 1: Load historical data (Custom Agent)
                DataLoaderAgent(),  # Herda de BaseAgent

                # Step 2: Prophet forecasting (Custom Agent)
                ProphetForecastingAgent(),  # Herda de BaseAgent

                # Step 3: Pipeline-weighted forecast (Custom Agent)
                PipelineWeightedAgent(),  # Herda de BaseAgent

                # Step 4: Ensemble (Custom Agent)
                EnsembleAgent()  # Herda de BaseAgent
            ]
        )
```

### **Performance Comparison: Custom Agent vs LLM Agent**

| Metric | Custom Agent (XGBoost) | LLM Agent (Gemini) | Improvement |
|--------|------------------------|-------------------|-------------|
| **Latency** | 5ms | 2000ms | **400x faster** |
| **Cost** | $0 | $0.03 | **Infinite savings** |
| **Throughput** | 200 req/sec | 0.5 req/sec | **400x higher** |
| **Accuracy** | 87% (deterministic) | 82% (variable) | **+5% better** |
| **Offline** | âœ… Yes | âŒ No | **Works offline** |

### **Best Practices: When to Use Custom Agents**

#### âœ… **USE Custom Agents (BaseAgent) for:**
1. **ML Model Inference**: XGBoost, Random Forest, Prophet, ARIMA
2. **Statistical Calculations**: Mean, median, percentiles, correlations
3. **Mathematical Optimization**: Linear programming, gradient descent
4. **Data Transformations**: ETL, aggregations, joins
5. **Deterministic Logic**: If-then rules, scoring formulas
6. **API Integrations**: REST/gRPC calls with deterministic payloads

#### âŒ **DO NOT Use Custom Agents for:**
1. **Natural Language Generation**: Email copy, reports, summaries
2. **Natural Language Understanding**: Intent detection, sentiment analysis
3. **Creative Tasks**: Brainstorming, ideation, strategy
4. **Complex Reasoning**: Multi-step planning, causal inference
5. **Ambiguous Tasks**: Tasks requiring interpretation

---

## ðŸ§© ESTRATÃ‰GIAS DE COMPOSIÃ‡ÃƒO ADK

### **PadrÃ£o 1: Research Pipeline**

```
DeepResearchAgent = SequentialAgent(
    agents=[
        QueryDecompositionAgent(LlmAgent),
        ParallelLiteratureSearchAgent(ParallelAgent),
        EvidenceSynthesisAgent(LoopAgent),
        ReportGenerationAgent(LlmAgent)
    ]
)
```

### **PadrÃ£o 2: Medical Triage**

```
ClinicalTriageAgent = CoordinatorAgent(
    specialist_agents={
        "cardiac": CardiacRiskAgent(ParallelAgent),
        "sepsis": SepsisRiskAgent(LlmAgent),
        "stroke": StrokeRiskAgent(LlmAgent),
        "general": GeneralTriageAgent(LlmAgent)
    },
    routing_logic=symptom_based_routing
)
```

### **PadrÃ£o 3: Financial Analysis**

```
FinancialAnalystAgent = SequentialAgent(
    agents=[
        DataFetchingAgent(ParallelAgent),  # Simultaneous: B3 + CVM + Bloomberg
        FinancialRatioAgent(LlmAgent),
        ValuationAgent(LlmAgent),
        RiskAssessmentAgent(ReflectiveAgent)
    ]
)
```

### **PadrÃ£o 4: AB Test Management**

```
ABTestAgent = LoopAgent(
    condition=lambda state: not experiment_concluded(state),
    agents=[
        ExperimentDesignAgent(LlmAgent),
        DataCollectionAgent(ParallelAgent),
        StatisticalTestAgent(LlmAgent),
        DecisionAgent(ReflectiveAgent)
    ]
)
```

---

## ðŸ—ï¸ PADRÃ•ES DE PROJETO PYTHON

### **Hexagonal Architecture para Agents**

```
ventros-ai/
â”œâ”€â”€ domain/                          # CORE (Business Logic)
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”œâ”€â”€ research_agent.py        # DeepResearchAgent
â”‚   â”‚   â”œâ”€â”€ meta_analysis_agent.py
â”‚   â”‚   â”œâ”€â”€ clinical_triage_agent.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ models/                      # Value Objects, Entities
â”‚   â”‚   â”œâ”€â”€ knowledge_scope.py
â”‚   â”‚   â”œâ”€â”€ research_query.py
â”‚   â”‚   â”œâ”€â”€ clinical_symptom.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ services/                    # Domain Services
â”‚       â”œâ”€â”€ memory_service.py
â”‚       â”œâ”€â”€ citation_service.py
â”‚       â””â”€â”€ risk_scoring_service.py
â”‚
â”œâ”€â”€ ports/                           # INTERFACES
â”‚   â”œâ”€â”€ inbound/                     # Primary Ports (Use Cases)
â”‚   â”‚   â”œâ”€â”€ research_use_cases.py
â”‚   â”‚   â”œâ”€â”€ analysis_use_cases.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ outbound/                    # Secondary Ports (Repositories, External)
â”‚       â”œâ”€â”€ memory_repository.py
â”‚       â”œâ”€â”€ document_repository.py
â”‚       â”œâ”€â”€ mcp_client_interface.py
â”‚       â””â”€â”€ llm_provider_interface.py
â”‚
â”œâ”€â”€ adapters/                        # IMPLEMENTATIONS
â”‚   â”œâ”€â”€ inbound/                     # Primary Adapters (Controllers)
â”‚   â”‚   â”œâ”€â”€ grpc/
â”‚   â”‚   â”‚   â””â”€â”€ research_handler.py
â”‚   â”‚   â”œâ”€â”€ rest/
â”‚   â”‚   â”‚   â””â”€â”€ analysis_handler.py
â”‚   â”‚   â””â”€â”€ rabbitmq/
â”‚   â”‚       â””â”€â”€ event_consumer.py
â”‚   â”œâ”€â”€ outbound/                    # Secondary Adapters (Implementations)
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â””â”€â”€ grpc_memory_adapter.py
â”‚   â”‚   â”œâ”€â”€ mcp/
â”‚   â”‚   â”‚   â””â”€â”€ http_mcp_client.py
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ vertex_ai_adapter.py
â”‚   â”‚   â”‚   â””â”€â”€ gemini_adapter.py
â”‚   â”‚   â””â”€â”€ storage/
â”‚   â”‚       â””â”€â”€ gcs_document_storage.py
â”‚
â”œâ”€â”€ application/                     # APPLICATION LAYER
â”‚   â”œâ”€â”€ commands/                    # Command Handlers (CQRS)
â”‚   â”‚   â”œâ”€â”€ start_research_command.py
â”‚   â”‚   â”œâ”€â”€ analyze_contract_command.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ queries/                     # Query Handlers (CQRS)
â”‚   â”‚   â”œâ”€â”€ get_research_status_query.py
â”‚   â”‚   â”œâ”€â”€ list_analyses_query.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ workflows/                   # Temporal Workflows
â”‚       â”œâ”€â”€ deep_research_workflow.py
â”‚       â”œâ”€â”€ meta_analysis_workflow.py
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ infrastructure/                  # INFRASTRUCTURE
â”‚   â”œâ”€â”€ observability/
â”‚   â”‚   â”œâ”€â”€ phoenix_tracer.py
â”‚   â”‚   â””â”€â”€ metrics_collector.py
â”‚   â”œâ”€â”€ messaging/
â”‚   â”‚   â””â”€â”€ rabbitmq_client.py
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ settings.py
â”‚
â””â”€â”€ tests/                           # TESTS
    â”œâ”€â”€ unit/
    â”‚   â”œâ”€â”€ domain/
    â”‚   â””â”€â”€ adapters/
    â”œâ”€â”€ integration/
    â”‚   â”œâ”€â”€ mcp_client_test.py
    â”‚   â””â”€â”€ memory_service_test.py
    â””â”€â”€ e2e/
        â””â”€â”€ research_flow_test.py
```

### **Dependency Injection Container**

```python
# infrastructure/di_container.py

from dependency_injector import containers, providers
from domain.services.memory_service import MemoryService
from adapters.outbound.memory.grpc_memory_adapter import GRPCMemoryAdapter
from adapters.outbound.mcp.http_mcp_client import HTTPMCPClient
from domain.agents.research_agent import DeepResearchAgent

class AgentContainer(containers.DeclarativeContainer):
    """Dependency Injection Container"""

    config = providers.Configuration()

    # Outbound Adapters (Infrastructure)
    memory_adapter = providers.Singleton(
        GRPCMemoryAdapter,
        host=config.memory_service.host,
        port=config.memory_service.port
    )

    mcp_client = providers.Singleton(
        HTTPMCPClient,
        base_url=config.mcp_server.url,
        auth_token=config.mcp_server.token
    )

    # Domain Services
    memory_service = providers.Factory(
        MemoryService,
        memory_adapter=memory_adapter
    )

    # Agents
    research_agent = providers.Factory(
        DeepResearchAgent,
        memory_service=memory_service,
        mcp_client=mcp_client,
        model=config.llm.model_name
    )

    # ... outros agents

# Usage in application
from infrastructure.di_container import AgentContainer

container = AgentContainer()
container.config.from_yaml("config.yaml")

research_agent = container.research_agent()
result = await research_agent.run_async("What are the latest studies on X?")
```

### **CQRS Pattern**

```python
# application/commands/start_research_command.py

from dataclasses import dataclass
from typing import Optional

@dataclass
class StartResearchCommand:
    """Command to start a deep research task"""
    tenant_id: str
    contact_id: str
    research_question: str
    knowledge_scope: dict
    requester_id: str
    priority: str = "medium"

class StartResearchCommandHandler:
    """Handles StartResearchCommand"""

    def __init__(
        self,
        research_agent: DeepResearchAgent,
        event_bus: EventBus,
        research_repository: ResearchRepository
    ):
        self.research_agent = research_agent
        self.event_bus = event_bus
        self.research_repository = research_repository

    async def handle(self, command: StartResearchCommand) -> str:
        """Execute command and return research_id"""

        # 1. Create research entity
        research = Research.create(
            tenant_id=command.tenant_id,
            question=command.research_question,
            requester_id=command.requester_id
        )

        # 2. Persist
        await self.research_repository.save(research)

        # 3. Publish event (async processing)
        await self.event_bus.publish(ResearchStarted(
            research_id=research.id,
            tenant_id=command.tenant_id,
            question=command.research_question
        ))

        return research.id


# application/queries/get_research_status_query.py

@dataclass
class GetResearchStatusQuery:
    """Query to get research status"""
    research_id: str
    tenant_id: str

class GetResearchStatusQueryHandler:
    """Handles GetResearchStatusQuery"""

    def __init__(self, research_repository: ResearchRepository):
        self.research_repository = research_repository

    async def handle(self, query: GetResearchStatusQuery) -> dict:
        """Execute query and return status"""

        research = await self.research_repository.get(
            research_id=query.research_id,
            tenant_id=query.tenant_id
        )

        return {
            "research_id": research.id,
            "status": research.status,  # pending, processing, completed, failed
            "progress": research.progress,  # 0-100
            "created_at": research.created_at,
            "completed_at": research.completed_at,
            "result_summary": research.summary if research.is_completed() else None
        }
```

### **Event-Driven Architecture**

```python
# domain/events.py

from dataclasses import dataclass
from datetime import datetime
from typing import Optional

@dataclass
class DomainEvent:
    """Base domain event"""
    event_id: str
    event_type: str
    tenant_id: str
    timestamp: datetime
    metadata: dict

@dataclass
class ResearchStarted(DomainEvent):
    research_id: str
    question: str

@dataclass
class LiteratureSearchCompleted(DomainEvent):
    research_id: str
    documents_found: int

@dataclass
class ResearchCompleted(DomainEvent):
    research_id: str
    summary: str
    citations_count: int


# application/event_handlers.py

class ResearchEventHandler:
    """Handles research-related events"""

    def __init__(self, research_agent: DeepResearchAgent):
        self.research_agent = research_agent

    async def on_research_started(self, event: ResearchStarted):
        """Start research processing"""

        result = await self.research_agent.run_async(
            user_input=event.question,
            session=create_session(event.tenant_id, event.research_id)
        )

        # Publish completion event
        await self.event_bus.publish(ResearchCompleted(
            research_id=event.research_id,
            summary=result.summary,
            citations_count=len(result.citations)
        ))
```

---

## ðŸ“š IMPLEMENTAÃ‡ÃƒO DETALHADA

### **DeepResearchAgent - Complete Implementation**

```python
# domain/agents/research_agent.py

from adk import LlmAgent, SequentialAgent, ParallelAgent, LoopAgent, Tool
from vertexai.generative_models import GenerativeModel
from typing import List, Dict
import asyncio

class QueryDecompositionAgent(LlmAgent):
    """DecompÃµe query complexa em sub-questÃµes"""

    def __init__(self):
        super().__init__(
            name="query_decomposer",
            model=GenerativeModel("gemini-2.0-flash-exp"),
            instruction="""
            VocÃª Ã© um especialista em decomposiÃ§Ã£o de questÃµes de pesquisa.

            Sua tarefa:
            1. Analise a questÃ£o de pesquisa fornecida
            2. Identifique os conceitos-chave
            3. Decomponha em 3-5 sub-questÃµes especÃ­ficas e respondÃ­veis
            4. Para cada sub-questÃ£o, sugira termos de busca (keywords, MeSH terms)

            Formato de saÃ­da (JSON):
            {
                "main_question": "...",
                "concepts": ["conceito1", "conceito2"],
                "sub_questions": [
                    {
                        "question": "...",
                        "search_terms": ["term1", "term2"],
                        "priority": "high|medium|low"
                    }
                ]
            }
            """,
            tools=[]
        )


class LiteratureSearchAgent(ParallelAgent):
    """Busca paralela em mÃºltiplas fontes"""

    def __init__(self, mcp_client):
        # Define sub-agents para busca paralela
        pubmed_agent = LlmAgent(
            name="pubmed_searcher",
            model=GenerativeModel("gemini-2.0-flash-exp"),
            instruction="Search PubMed using MeSH terms and keywords",
            tools=[
                Tool(
                    name="search_pubmed",
                    func=lambda query: mcp_client.call_tool("search_documents", {
                        "query": query,
                        "content_types": ["research_paper"],
                        "sources": ["pubmed"],
                        "limit": 50
                    })
                )
            ]
        )

        google_scholar_agent = LlmAgent(
            name="scholar_searcher",
            model=GenerativeModel("gemini-2.0-flash-exp"),
            instruction="Search Google Scholar for academic papers",
            tools=[
                Tool(
                    name="search_scholar",
                    func=lambda query: mcp_client.call_tool("search_documents", {
                        "query": query,
                        "content_types": ["research_paper", "article"],
                        "sources": ["google_scholar"],
                        "limit": 50
                    })
                )
            ]
        )

        internal_docs_agent = LlmAgent(
            name="internal_searcher",
            model=GenerativeModel("gemini-2.0-flash-exp"),
            instruction="Search internal document database",
            tools=[
                Tool(
                    name="search_internal",
                    func=lambda query: mcp_client.call_tool("search_documents", {
                        "query": query,
                        "content_types": ["document", "research_paper"],
                        "limit": 50
                    })
                )
            ]
        )

        super().__init__(
            name="literature_search",
            agents=[pubmed_agent, google_scholar_agent, internal_docs_agent]
        )


class EvidenceSynthesisAgent(LoopAgent):
    """Sintetiza evidÃªncias iterativamente atÃ© convergÃªncia"""

    def __init__(self):
        synthesis_agent = LlmAgent(
            name="synthesizer",
            model=GenerativeModel("gemini-2.0-flash-exp"),
            instruction="""
            VocÃª Ã© um especialista em sÃ­ntese de evidÃªncias cientÃ­ficas.

            Sua tarefa:
            1. Analise os papers encontrados
            2. Extraia findings principais de cada paper
            3. Identifique consenso e contradiÃ§Ãµes
            4. Avalie qualidade das evidÃªncias (GRADE approach)
            5. Sintetize em uma narrativa coerente

            Para cada paper, extraia:
            - Study design (RCT, cohort, case-control, etc)
            - Sample size
            - Main findings
            - Effect size (if quantitative)
            - Limitations
            - Quality score (0-10)

            Identifique:
            - Consensus findings (reported in â‰¥3 papers)
            - Contradictory findings
            - Gaps in evidence
            """,
            tools=[
                Tool(name="extract_citations", func=extract_citation_network),
                Tool(name="quality_score", func=calculate_study_quality),
                Tool(name="detect_contradictions", func=find_contradictions)
            ]
        )

        # Loop atÃ© convergÃªncia ou max 3 iteraÃ§Ãµes
        super().__init__(
            name="evidence_synthesis",
            agent=synthesis_agent,
            condition=lambda state: (
                state.get("iteration", 0) < 3 and
                state.get("confidence", 0) < 0.9
            )
        )


class ReportGenerationAgent(LlmAgent):
    """Gera relatÃ³rio estruturado final"""

    def __init__(self):
        super().__init__(
            name="report_generator",
            model=GenerativeModel("gemini-2.0-flash-exp"),
            instruction="""
            VocÃª Ã© um redator cientÃ­fico especializado.

            Gere um relatÃ³rio estruturado seguindo o formato IMRAD:

            # Introduction
            - Background e contexto
            - Pergunta de pesquisa
            - Objetivos

            # Methods
            - EstratÃ©gia de busca
            - CritÃ©rios de inclusÃ£o/exclusÃ£o
            - Fontes consultadas
            - PerÃ­odo de busca

            # Results
            - NÃºmero de papers encontrados
            - Papers incluÃ­dos apÃ³s triagem
            - SÃ­ntese dos findings principais
            - Tabela de evidÃªncias

            # Discussion
            - InterpretaÃ§Ã£o dos resultados
            - Consenso na literatura
            - ContradiÃ§Ãµes e limitaÃ§Ãµes
            - ImplicaÃ§Ãµes prÃ¡ticas

            # References
            - Lista completa de citaÃ§Ãµes (formato APA)

            Use linguagem clara, precisa e acadÃªmica.
            Cite adequadamente (formato: Autor et al., Ano).
            """,
            tools=[
                Tool(name="format_citations", func=format_apa_citations),
                Tool(name="create_evidence_table", func=generate_evidence_table)
            ]
        )


class DeepResearchAgent(SequentialAgent):
    """
    Agent de pesquisa profunda com pipeline sequencial

    Pattern: QueryDecomposition â†’ ParallelSearch â†’ EvidenceSynthesis â†’ ReportGeneration
    """

    def __init__(self, memory_service, mcp_client):
        super().__init__(
            name="deep_research_agent",
            agents=[
                QueryDecompositionAgent(),
                LiteratureSearchAgent(mcp_client),
                EvidenceSynthesisAgent(),
                ReportGenerationAgent()
            ]
        )

        self.memory_service = memory_service
        self.mcp_client = mcp_client

    async def run_async(
        self,
        user_input: str,
        session: Session
    ) -> ResearchResult:
        """
        Execute deep research pipeline

        Args:
            user_input: Research question
            session: ADK session with state

        Returns:
            ResearchResult with complete report
        """

        # Get context from memory (optional, for personalized research)
        unified_context = await self.memory_service.get_unified_context(
            tenant_id=session.state["tenant_id"],
            contact_id=session.state.get("contact_id"),
            knowledge_scope={
                "lookback_days": 730,  # 2 years
                "include_documents": True,
                "include_research_history": True
            }
        )

        # Add context to session
        session.state["previous_research"] = unified_context.get("research_history", [])

        # Execute sequential pipeline
        result = await super().run_async(user_input, session)

        # Parse result
        report = result.content

        return ResearchResult(
            research_id=session.state.get("research_id"),
            question=user_input,
            report=report,
            citations=extract_citations_from_report(report),
            confidence=result.metadata.get("confidence", 0.8),
            papers_analyzed=result.metadata.get("papers_count", 0)
        )


# Helper functions

def extract_citation_network(papers: List[Dict]) -> Dict:
    """Extract citation network from papers"""
    # Implementation using NetworkX or similar
    pass

def calculate_study_quality(paper: Dict) -> float:
    """Calculate quality score using validated instruments"""
    # Implementation: Newcastle-Ottawa Scale, GRADE, etc
    pass

def find_contradictions(papers: List[Dict]) -> List[Dict]:
    """Find contradictory findings across papers"""
    # Implementation using semantic similarity
    pass

def format_apa_citations(citations: List[Dict]) -> str:
    """Format citations in APA style"""
    # Implementation
    pass

def generate_evidence_table(papers: List[Dict]) -> str:
    """Generate evidence table in markdown"""
    # Implementation
    pass
```

### **Usage Example**

```python
# application/commands/start_research_command.py

from infrastructure.di_container import AgentContainer

# Initialize container
container = AgentContainer()
container.config.from_yaml("config.yaml")

# Get research agent
research_agent = container.research_agent()

# Create session
from adk import Session

session = Session(state={
    "tenant_id": "tenant-123",
    "contact_id": "contact-456",
    "research_id": "research-789"
})

# Execute research
result = await research_agent.run_async(
    user_input="What are the latest evidence-based interventions for reducing customer churn in SaaS companies?",
    session=session
)

print(result.report)
# Output:
# # Introduction
# Customer churn in SaaS companies is a critical metric...
#
# # Methods
# We searched PubMed, Google Scholar, and internal databases...
# Search terms: "customer churn", "SaaS retention", "churn prediction"...
#
# # Results
# We identified 47 relevant papers published between 2023-2025...
# Key findings:
# 1. Proactive engagement reduces churn by 32% (95% CI: 24-40%, p<0.001)
# 2. Personalized onboarding increases retention by 28%...
# ...
```

---

## ðŸ“ REORGANIZAÃ‡ÃƒO DE DOCUMENTAÃ‡ÃƒO

### **Estrutura Proposta**

```
/docs
â”œâ”€â”€ architecture/                    # Arquitetura tÃ©cnica
â”‚   â”œâ”€â”€ AI_ARCHITECTURE_EXECUTIVE_SUMMARY.md
â”‚   â”œâ”€â”€ AI_MEMORY_GO_ARCHITECTURE.md
â”‚   â”œâ”€â”€ PYTHON_ADK_ARCHITECTURE.md
â”‚   â”œâ”€â”€ MCP_SERVER_COMPLETE.md
â”‚   â””â”€â”€ INTEGRATION_PLAN_MEMORY_GROUPS_DOCS.md
â”‚
â”œâ”€â”€ agents/                          # CatÃ¡logo de agentes
â”‚   â”œâ”€â”€ README.md                    # Overview
â”‚   â”œâ”€â”€ AGENT_PRESETS_CATALOG.md    # Este documento
â”‚   â”œâ”€â”€ patterns/
â”‚   â”‚   â”œâ”€â”€ coordinator_pattern.md
â”‚   â”‚   â”œâ”€â”€ sequential_pattern.md
â”‚   â”‚   â”œâ”€â”€ parallel_pattern.md
â”‚   â”‚   â””â”€â”€ loop_pattern.md
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ research_agent_example.md
â”‚       â”œâ”€â”€ medical_agent_example.md
â”‚       â””â”€â”€ financial_agent_example.md
â”‚
â”œâ”€â”€ api/                             # API documentation
â”‚   â”œâ”€â”€ grpc_api.md
â”‚   â”œâ”€â”€ rest_api.md
â”‚   â””â”€â”€ mcp_tools_reference.md
â”‚
â””â”€â”€ deployment/                      # Deployment guides
    â”œâ”€â”€ python_service_deployment.md
    â”œâ”€â”€ go_service_deployment.md
    â””â”€â”€ monitoring_setup.md

/guides                              # Guias prÃ¡ticos
â”œâ”€â”€ quickstart/
â”‚   â”œâ”€â”€ 01_setup.md
â”‚   â”œâ”€â”€ 02_create_first_agent.md
â”‚   â””â”€â”€ 03_deploy_to_production.md
â”‚
â”œâ”€â”€ tutorials/
â”‚   â”œâ”€â”€ building_research_agent.md
â”‚   â”œâ”€â”€ building_medical_triage_agent.md
â”‚   â”œâ”€â”€ building_financial_analyst.md
â”‚   â””â”€â”€ building_custom_agent.md
â”‚
â”œâ”€â”€ best_practices/
â”‚   â”œâ”€â”€ agent_design_patterns.md
â”‚   â”œâ”€â”€ knowledge_scope_configuration.md
â”‚   â”œâ”€â”€ prompt_engineering.md
â”‚   â””â”€â”€ cost_optimization.md
â”‚
â””â”€â”€ troubleshooting/
    â”œâ”€â”€ common_issues.md
    â”œâ”€â”€ performance_tuning.md
    â””â”€â”€ debugging_agents.md

/root (fica na raiz)
â”œâ”€â”€ README.md                        # Overview do projeto
â”œâ”€â”€ CONTRIBUTING.md                  # Como contribuir
â”œâ”€â”€ TODO.md                          # Roadmap
â”œâ”€â”€ DEV_GUIDE.md                     # Setup dev environment
â””â”€â”€ CHANGELOG.md                     # HistÃ³rico de mudanÃ§as
```

---

## ðŸŽ¯ RESUMO EXECUTIVO

### **Agentes Propostos (22 novos)**

#### **Research & Analysis (3)**
1. **DeepResearchAgent** - Pesquisa profunda com mÃºltiplas fontes
2. **MetaAnalysisAgent** - AnÃ¡lise estatÃ­stica agregada (forest plots, funnel plots)
3. **ScientificHypothesisAgent** - GeraÃ§Ã£o e validaÃ§Ã£o de hipÃ³teses

#### **Medical & Healthcare (2)**
4. **ClinicalTriageAgent** - Triagem administrativa (HEART, qSOFA, FAST scores)
5. **MedicalLiteratureAgent** - EspecializaÃ§Ã£o mÃ©dica do DeepResearchAgent

#### **Legal & Compliance (2)**
6. **ContractAnalyzerAgent** - AnÃ¡lise de clÃ¡usulas e riscos contratuais
7. **LegalResearchAgent** - Pesquisa jurisprudencial (STF, STJ, doutrina)

#### **Financial & Investment (2)**
8. **FinancialAnalystAgent** - AnÃ¡lise fundamentalista (DCF, ratios, valuation)
9. **InvestmentPortfolioAgent** - OtimizaÃ§Ã£o de portfÃ³lio (Markowitz, Black-Litterman)

#### **Data Science & Analytics (2)**
10. **DataAnalystAgent** - Natural Language to SQL + insights
11. **ABTestAgent** - Design de experimentos + monitoramento + decisÃ£o

#### **Product & UX (2)**
12. **UserFeedbackAnalyzerAgent** - ClassificaÃ§Ã£o + sÃ­ntese + roteamento
13. **ProductRoadmapAgent** - PriorizaÃ§Ã£o (RICE) + strategic fit

#### **Customer Success (1)**
14. **ChurnPredictionAgent** - ML prediction + intervention routing

#### **Marketing Automation (5)** âœ¨ NOVO
15. **CampaignOrchestratorAgent** - Design, execuÃ§Ã£o e otimizaÃ§Ã£o de campanhas multi-step (integra Campaign, Sequence, Broadcast)
16. **PersonalizationAgent** - Hyper-personalizaÃ§Ã£o 1-to-1 em escala (4 layers: basic, behavioral, predictive, contextual)
17. **ABMAgent** - Account-Based Marketing com scoring ICP + multi-channel orchestration (Email, LinkedIn, WhatsApp, Ads)
18. **ContentGeneratorAgent** - GeraÃ§Ã£o de conteÃºdo multi-formato (Email, WhatsApp, SMS, Social) por funnel stage (TOFU/MOFU/BOFU)
19. **FunnelOptimizationAgent** - DetecÃ§Ã£o de bottlenecks em Pipeline/Sequence + auto-fix orchestration

#### **Sales Enablement (3)** âœ¨ NOVO
20. **LeadScoringAgent** - ML predictive scoring (XGBoost) com 4 dimensÃµes (demographic, firmographic, behavioral, intent)
21. **SalesForecastingAgent** - Ensemble forecasting (historical + pipeline-weighted + AI Prophet) + at-risk deal detection
22. **DealAssistantAgent** - AnÃ¡lise de buying committee + competitive intelligence + playbook orchestration

### **Total: 34 agentes (12 existentes + 22 novos)**

### **IntegraÃ§Ã£o Ventros CRM**

Os 8 novos agentes de Marketing & Sales foram projetados especificamente para integrar com as estruturas do Ventros:

- **Campaign Aggregate**: CampaignOrchestratorAgent, FunnelOptimizationAgent
- **Sequence**: CampaignOrchestratorAgent, PersonalizationAgent, FunnelOptimizationAgent
- **Pipeline**: LeadScoringAgent, SalesForecastingAgent, DealAssistantAgent, FunnelOptimizationAgent
- **Contact**: LeadScoringAgent, PersonalizationAgent, ABMAgent, DealAssistantAgent
- **Session**: DealAssistantAgent, PersonalizationAgent
- **Broadcast**: CampaignOrchestratorAgent, ContentGeneratorAgent, PersonalizationAgent
- **Contact_list**: ABMAgent, FunnelOptimizationAgent

### **EstratÃ©gias ADK Usadas**

- **SequentialAgent**: DeepResearchAgent, ContractAnalyzerAgent, FinancialAnalystAgent, CampaignOrchestratorAgent, FunnelOptimizationAgent
- **ParallelAgent**: LiteratureSearchAgent, DataFetchingAgent, RiskScoringAgent, PersonalizationAgent, ABMAgent, LeadScoringAgent, SalesForecastingAgent
- **LoopAgent**: EvidenceSynthesisAgent, QualityAssuranceAgent, ABTestAgent, CampaignOrchestratorAgent (optimization), ABMAgent (nurturing), FunnelOptimizationAgent (monitoring)
- **CoordinatorAgent**: ClinicalTriageAgent, UserFeedbackAnalyzerAgent, ABMAgent (orchestration), FunnelOptimizationAgent (fix routing), LeadScoringAgent (routing), DealAssistantAgent (playbook)
- **ReflectiveAgent**: ScientificHypothesisAgent, RiskAssessmentAgent, StrategicFitAgent, PersonalizationAgent (quality check), ContentGeneratorAgent (brand consistency), SalesForecastingAgent (sanity check)

### **PadrÃµes Python**

- âœ… Hexagonal Architecture (Ports & Adapters)
- âœ… CQRS (Command Query Responsibility Segregation)
- âœ… Event-Driven Architecture
- âœ… Dependency Injection
- âœ… Domain-Driven Design

### **âš¡ Diferencial: Custom Agents (BaseAgent)**

**IMPORTANTE**: O ADK permite criar **Custom Agents** herdando de `BaseAgent` que executam cÃ³digo Python diretamente **SEM usar LLMs**. Isso Ã© um diferencial poderoso para:

- **LeadScoringAgent**: XGBoost inference em 5ms (400x mais rÃ¡pido que LLM)
- **SalesForecastingAgent**: Prophet/ARIMA para forecasting determinÃ­stico
- **ABMAgent**: Account scoring com lÃ³gica determinÃ­stica
- **FunnelOptimizationAgent**: AnÃ¡lise de mÃ©tricas sem LLM calls

**Performance**: Custom Agents sÃ£o 100-1000x mais rÃ¡pidos e tÃªm custo $0 vs $0.01-0.10 por LLM call.

**Pattern HÃ­brido**: Combine Custom Agents (cÃ¡lculos rÃ¡pidos) + LLM Agents (raciocÃ­nio/geraÃ§Ã£o de texto) para melhor resultado.

---

### **Status de ImplementaÃ§Ã£o** âœ…

**CatÃ¡logo Completo:**
- âœ… 12 agentes existentes (base)
- âœ… 14 agentes profissionais (Research, Medical, Legal, Financial, Data Science, Product, Customer Success)
- âœ… 8 agentes Marketing & Sales (Campaign, Personalization, ABM, Content, Funnel, Lead Scoring, Forecasting, Deal Assistant)

**Total: 34 agentes documentados**

**PrÃ³ximos Passos Sugeridos:**
1. Implementar agentes prioritÃ¡rios em Python (CampaignOrchestratorAgent, LeadScoringAgent, PersonalizationAgent)
2. Criar testes de integraÃ§Ã£o com Ventros CRM (Campaign, Sequence, Pipeline)
3. Desenvolver dashboards de performance para Marketing & Sales agents
4. Configurar observability (Phoenix tracing) para novos agents
